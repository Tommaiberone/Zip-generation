{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch pandas lightning trl\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 999\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Custom Dataset\n",
    "class TextHexDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['text']\n",
    "\n",
    "        # Convert hex into tensor\n",
    "        hex_data = self.dataframe.iloc[idx]['deflate_hex']\n",
    "        hex_data = [int(x, 16) for x in hex_data]\n",
    "        \n",
    "        #pad to reach 512\n",
    "        padded_hex_data = hex_data + [0] * (512 - len(hex_data))\n",
    "        tensor_hex_data = torch.tensor(padded_hex_data)\n",
    "        return text, tensor_hex_data\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../../Datasets/new_dataset_deflate.csv')\n",
    "\n",
    "# Create datasets\n",
    "dataset = TextHexDataset(df)\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders with batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]VALIDATION_STEP: loss = 9.817407608032227\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  8.26it/s]VALIDATION_STEP: loss = 9.684063911437988\n",
      "Epoch 0:   0%|          | 0/2500 [00:00<?, ?it/s]                          TRAINING_STEP: loss = 10.058135032653809\n",
      "Epoch 0:   0%|          | 1/2500 [00:00<08:46,  4.75it/s]TRAINING_STEP: loss = 23061.859375\n",
      "Epoch 0:   0%|          | 2/2500 [00:01<35:41,  1.17it/s]TRAINING_STEP: loss = 127007.265625\n",
      "Epoch 0:   0%|          | 3/2500 [00:02<41:09,  1.01it/s]TRAINING_STEP: loss = 41278.96484375\n",
      "Epoch 0:   0%|          | 4/2500 [00:04<45:22,  0.92it/s]TRAINING_STEP: loss = 26156.712890625\n",
      "Epoch 0:   0%|          | 5/2500 [00:05<48:40,  0.85it/s]TRAINING_STEP: loss = 28129.978515625\n",
      "Epoch 0:   0%|          | 6/2500 [00:07<51:24,  0.81it/s]TRAINING_STEP: loss = 24468.435546875\n",
      "Epoch 0:   0%|          | 7/2500 [00:08<52:31,  0.79it/s]TRAINING_STEP: loss = 1295.296142578125\n",
      "Epoch 0:   0%|          | 8/2500 [00:10<53:38,  0.77it/s]TRAINING_STEP: loss = 15498.5341796875\n",
      "Epoch 0:   0%|          | 9/2500 [00:11<54:18,  0.76it/s]TRAINING_STEP: loss = 2001.111572265625\n",
      "Epoch 0:   0%|          | 10/2500 [00:13<54:48,  0.76it/s]TRAINING_STEP: loss = 4690.474609375\n",
      "Epoch 0:   0%|          | 11/2500 [00:14<55:29,  0.75it/s]TRAINING_STEP: loss = 8844.748046875\n",
      "Epoch 0:   0%|          | 12/2500 [00:16<55:56,  0.74it/s]TRAINING_STEP: loss = 3332.68310546875\n",
      "Epoch 0:   1%|          | 13/2500 [00:17<56:40,  0.73it/s]TRAINING_STEP: loss = 131.0418243408203\n",
      "Epoch 0:   1%|          | 14/2500 [00:19<56:45,  0.73it/s]TRAINING_STEP: loss = 2773.540771484375\n",
      "Epoch 0:   1%|          | 15/2500 [00:20<57:05,  0.73it/s]TRAINING_STEP: loss = 4415.9892578125\n",
      "Epoch 0:   1%|          | 16/2500 [00:22<57:24,  0.72it/s]TRAINING_STEP: loss = 2308.145751953125\n",
      "Epoch 0:   1%|          | 17/2500 [00:23<57:50,  0.72it/s]TRAINING_STEP: loss = 517.4038696289062\n",
      "Epoch 0:   1%|          | 18/2500 [00:25<58:17,  0.71it/s]TRAINING_STEP: loss = 629.7177124023438\n",
      "Epoch 0:   1%|          | 19/2500 [00:26<58:32,  0.71it/s]TRAINING_STEP: loss = 1953.7979736328125\n",
      "Epoch 0:   1%|          | 20/2500 [00:28<58:59,  0.70it/s]TRAINING_STEP: loss = 2358.364501953125\n",
      "Epoch 0:   1%|          | 21/2500 [00:30<59:11,  0.70it/s]"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "class TransformerModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "        self.transformer = BartModel.from_pretrained('facebook/bart-base')\n",
    "\n",
    "        #set the transformer padding character to 0\n",
    "        self.transformer.config.pad_token_id = 0\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128 * 768, 512)\n",
    "\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        # Tokenize the text\n",
    "        input_ids = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        attention_mask = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).attention_mask.to(device)\n",
    "\n",
    "        # Pad input_ids and attention_mask to a fixed length of 512\n",
    "        padded_input_ids = F.pad(input_ids, (0, 128 - input_ids.shape[1]), 'constant', 0)\n",
    "        padded_attention_mask = F.pad(attention_mask, (0, 128 - attention_mask.shape[1]), 'constant', 0)\n",
    "\n",
    "        # Ensure padding is on the device\n",
    "        padded_input_ids = padded_input_ids.to(device)\n",
    "        padded_attention_mask = padded_attention_mask.to(device)\n",
    "\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: padded_input_ids.shape = {padded_input_ids.shape}\")\n",
    "            print(f\"FORWARD: padded_attention_mask.shape = {padded_attention_mask.shape}\")\n",
    "\n",
    "        # Pass tokenized and padded text through the transformer\n",
    "        transformer_output = self.transformer(input_ids=padded_input_ids, attention_mask=padded_attention_mask).last_hidden_state\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: transformer_output.shape = {transformer_output.shape}\")\n",
    "\n",
    "        # Pooling over the sequence dimension\n",
    "        flattened_output = self.flatten(transformer_output)\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: flattened_output.shape = {flattened_output.shape}\")\n",
    "\n",
    "        # Apply the linear layer\n",
    "        final_output = self.linear(flattened_output)\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: final_output.shape = {final_output.shape}\")\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text, hex_data = batch\n",
    "\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP: text = {text}\")\n",
    "            print(f\"TRAINING_STEP: hex_data.shape = {hex_data.shape}\")\n",
    "\n",
    "        # Pass the text through the transformer\n",
    "        transformer_output = self.forward(text)\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP: transformer_output.shape = {transformer_output.shape}\")\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.loss(transformer_output, hex_data.float())\n",
    "        print(f\"TRAINING_STEP: loss = {loss}\")\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text, hex_data = batch\n",
    "        if DEBUG:\n",
    "            print(f\"VALIDATION_STEP: text = {text}\")\n",
    "            print(f\"VALIDATION_STEP: hex_data = {hex_data}\")\n",
    "        \n",
    "        # Pass the text through the transformer\n",
    "        transformer_output = self.forward(text)\n",
    "        if DEBUG:\n",
    "            print(f\"VALIDATION_STEP: transformer_output.shape = {transformer_output.shape}\")\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.loss(transformer_output, hex_data.float())\n",
    "        print(f\"VALIDATION_STEP: loss = {loss}\")\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.05)\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerModel()\n",
    "\n",
    "# load the weights\n",
    "model.load_state_dict(torch.load('bart_model.pt'))\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=5, enable_checkpointing=False, logger=False)\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights\n",
    "# torch.save(model.state_dict(), 'bart_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 789c2cab820d82311835a3876986a585769716865537864743669577626789c8a575867d788567877a27477675677776678ac934958a8896567977ab884a786a9798597b737777278aaa8a969855686a5766861855822554343552552534434544464435351233120100010101000000000000000000110100010011000001000010000000001000000001000001000100100000010000000000100000000000000000000000000001000000000100000010000000000100010000010000000001000100000000000000100000000000000100000100011000010000000000000000100000100000000000000100000000000100000000000010000000100001\n",
      "Gold: 789c1dccd10d83301004d156b600520865386281957c67e0ce48ee1e92df37d2cc0e1b70debc3ef445bee1ec8c4436048912b0e2e3c5529503d66e317e7cb4087d2ba1ffa26a65ca384188ecf69605fd688edc155855ed016233278f0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "# Test the model\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "def decimal_to_hexadecimal(decimal):    \n",
    "    hex_digits = \"0123456789abcdefghilmnopqrstuvz\"\n",
    "    return hex_digits[decimal]\n",
    "\n",
    "for text, gold in val_loader:\n",
    "\n",
    "    prediction = model.forward(text[0]).tolist()[0]\n",
    "\n",
    "    #round every elem in prediction\n",
    "    prediction = [round(x) for x in prediction]\n",
    "\n",
    "    #set to 0 every negative elem in prediction\n",
    "    prediction = [max(0, x) for x in prediction]\n",
    "\n",
    "    #convert every 0 to 0, 1 to 1, ... , 15 to f\n",
    "    hex_prediction = [decimal_to_hexadecimal(x) for x in prediction]\n",
    "\n",
    "    hex_gold = [decimal_to_hexadecimal(x) for x in gold[0].tolist()]\n",
    "\n",
    "    #convert to string\n",
    "    hex_prediction = ''.join(hex_prediction)\n",
    "    hex_gold = ''.join(hex_gold)\n",
    "\n",
    "    print(f\"Prediction: {hex_prediction}\")\n",
    "    print(f\"Gold: {hex_gold}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
