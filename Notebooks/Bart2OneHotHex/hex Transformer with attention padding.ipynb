{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7368959,"sourceType":"datasetVersion","datasetId":4281201}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install torch pandas lightning trl\n\nimport torch\nfrom torch import nn\nimport pytorch_lightning as pl\nfrom datasets import load_dataset, Dataset, DatasetDict\nimport pandas as pd\n\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BartForConditionalGeneration, BartTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-01-09T15:47:54.657745Z","iopub.execute_input":"2024-01-09T15:47:54.658706Z","iopub.status.idle":"2024-01-09T15:48:07.611195Z","shell.execute_reply.started":"2024-01-09T15:47:54.658670Z","shell.execute_reply":"2024-01-09T15:48:07.609879Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"SEED = 999\ntorch.manual_seed(SEED)\n\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n\n#set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# device = torch.device('cpu')\nprint(\"Device:\", device)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T15:48:07.613928Z","iopub.execute_input":"2024-01-09T15:48:07.614270Z","iopub.status.idle":"2024-01-09T15:48:07.621182Z","shell.execute_reply.started":"2024-01-09T15:48:07.614242Z","shell.execute_reply":"2024-01-09T15:48:07.620230Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\nimport torch.nn.functional as F\n\n# Custom Dataset\nclass TextHexDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        text = self.dataframe.iloc[idx]['text']\n\n        # Convert hex into tensor\n        hex_data = self.dataframe.iloc[idx]['deflate_hex']\n        hex_data = [int(x, 16) for x in hex_data]\n        \n        #pad to reach 256\n        padded_hex_data = hex_data + [20] * (256 - len(hex_data))\n        tensor_hex_data = torch.tensor(padded_hex_data)\n        return text, tensor_hex_data\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/shorthex2hex/shorthex2hex.csv')\n\n# Create datasets\ndataset = TextHexDataset(df)\n\n# Split the dataset\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n# DataLoaders with batch_size = 1\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers = 4)\nval_loader = DataLoader(val_dataset, batch_size=16, num_workers = 4)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T15:48:07.622322Z","iopub.execute_input":"2024-01-09T15:48:07.622589Z","iopub.status.idle":"2024-01-09T15:48:07.767511Z","shell.execute_reply.started":"2024-01-09T15:48:07.622546Z","shell.execute_reply":"2024-01-09T15:48:07.766690Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom transformers import BartTokenizer, BartModel\nimport torch\nimport torch.nn.functional as F\n\nDEBUG = False\n\n\nclass TransformerModel(pl.LightningModule):\n    def __init__(self):\n        super(TransformerModel, self).__init__()\n        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n        self.transformer = BartModel.from_pretrained('facebook/bart-base')\n\n        #set the transformer padding character to 0\n        self.transformer.config.pad_token_id = 20\n\n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 768, 256),\n            nn.Dropout(p=0.2)  # Example of adding a dropout\n            # You can add more layers here\n        )\n\n        self.loss = torch.nn.MSELoss()\n\n        self.to(device)\n\n    def forward(self, text):\n\n        # Tokenize the text\n        input_ids = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n        attention_mask = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).attention_mask.to(device)\n\n        # Pad input_ids and attention_mask to a fixed length of 256\n        padded_input_ids = F.pad(input_ids, (0, 128 - input_ids.shape[1]), 'constant', 20)\n        padded_attention_mask = F.pad(attention_mask, (0, 128 - attention_mask.shape[1]), 'constant', 20)\n\n        # Ensure padding is on the device\n        padded_input_ids = padded_input_ids.to(device)\n        padded_attention_mask = padded_attention_mask.to(device)\n\n        if DEBUG:\n            print(f\"FORWARD: padded_input_ids.shape = {padded_input_ids.shape}\")\n            print(f\"FORWARD: padded_attention_mask.shape = {padded_attention_mask.shape}\")\n\n        # Pass tokenized and padded text through the transformer\n        transformer_output = self.transformer(input_ids=padded_input_ids, attention_mask=padded_attention_mask).last_hidden_state\n        if DEBUG:\n            print(f\"FORWARD: transformer_output.shape = {transformer_output.shape}\")\n\n        # Apply the linear layer\n        final_output = self.encoder(transformer_output)\n        if DEBUG:\n            print(f\"FORWARD: final_output.shape = {final_output.shape}\")\n\n        return final_output\n    \n    def training_step(self, batch, batch_idx):\n        text, hex_data = batch\n\n        if DEBUG:\n            print(f\"TRAINING_STEP: text = {text}\")\n            print(f\"TRAINING_STEP: hex_data.shape = {hex_data.shape}\")\n\n        # Pass the text through the transformer\n        transformer_output = self.forward(text)\n        if DEBUG:\n            print(f\"TRAINING_STEP: transformer_output.shape = {transformer_output.shape}\")\n\n        # Calculate the loss\n        loss = self.loss(transformer_output, hex_data.float())\n        print(f\"TRAINING_STEP: loss = {loss}\")\n\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        text, hex_data = batch\n        if DEBUG:\n            print(f\"VALIDATION_STEP: text = {text}\")\n            print(f\"VALIDATION_STEP: hex_data = {hex_data}\")\n        \n        # Pass the text through the transformer\n        transformer_output = self.forward(text)\n        if DEBUG:\n            print(f\"VALIDATION_STEP: transformer_output.shape = {transformer_output.shape}\")\n\n        # Calculate the loss\n        loss = self.loss(transformer_output, hex_data.float())\n        print(f\"VALIDATION_STEP: loss = {loss}\")\n\n        return loss\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=1e-5)\n\n\n\nmodel = TransformerModel()\n\ntrainer = pl.Trainer(max_epochs=10, enable_checkpointing=False, logger=False)\ntrainer.fit(model, train_loader, val_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T15:51:58.656533Z","iopub.execute_input":"2024-01-09T15:51:58.656928Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"VALIDATION_STEP: loss = 330.49615478515625\nVALIDATION_STEP: loss = 323.34039306640625\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a237a82f7b824db8b5517d8f44e89805"}},"metadata":{}},{"name":"stdout","text":"TRAINING_STEP: loss = 328.6442565917969\nTRAINING_STEP: loss = 174.80923461914062\nTRAINING_STEP: loss = 37129.703125\nTRAINING_STEP: loss = 127.32447052001953\nTRAINING_STEP: loss = 4275.3837890625\nTRAINING_STEP: loss = 933.1063232421875\nTRAINING_STEP: loss = 565.965576171875\nTRAINING_STEP: loss = 89.78602600097656\nTRAINING_STEP: loss = 92.8228759765625\nTRAINING_STEP: loss = 102.99302673339844\nTRAINING_STEP: loss = 81.9697494506836\nTRAINING_STEP: loss = 86.85912322998047\nTRAINING_STEP: loss = 83.69911193847656\nTRAINING_STEP: loss = 87.74603271484375\nTRAINING_STEP: loss = 90.00677490234375\nTRAINING_STEP: loss = 82.52682495117188\nTRAINING_STEP: loss = 80.99491882324219\nTRAINING_STEP: loss = 92.05821228027344\nTRAINING_STEP: loss = 112.75422668457031\nTRAINING_STEP: loss = 95.71321868896484\nTRAINING_STEP: loss = 83.33638000488281\nTRAINING_STEP: loss = 81.85674285888672\nTRAINING_STEP: loss = 78.91610717773438\nTRAINING_STEP: loss = 83.66233825683594\nTRAINING_STEP: loss = 93.01873779296875\nTRAINING_STEP: loss = 104.70057678222656\nTRAINING_STEP: loss = 124.575439453125\nTRAINING_STEP: loss = 152.512939453125\nTRAINING_STEP: loss = 129.74349975585938\nTRAINING_STEP: loss = 83.66354370117188\nTRAINING_STEP: loss = 77.46302795410156\nTRAINING_STEP: loss = 101.53022766113281\nTRAINING_STEP: loss = 98.76842498779297\nTRAINING_STEP: loss = 78.43257904052734\nTRAINING_STEP: loss = 78.09808349609375\nTRAINING_STEP: loss = 95.01495361328125\nTRAINING_STEP: loss = 84.85244750976562\nTRAINING_STEP: loss = 74.04737854003906\nTRAINING_STEP: loss = 91.8046646118164\nTRAINING_STEP: loss = 91.1564712524414\nTRAINING_STEP: loss = 75.28135681152344\nTRAINING_STEP: loss = 79.70085144042969\nTRAINING_STEP: loss = 84.19556427001953\nTRAINING_STEP: loss = 70.58092498779297\nTRAINING_STEP: loss = 78.90675354003906\nTRAINING_STEP: loss = 87.29116821289062\nTRAINING_STEP: loss = 80.79464721679688\nTRAINING_STEP: loss = 71.96480560302734\nTRAINING_STEP: loss = 81.48076629638672\nTRAINING_STEP: loss = 79.46533203125\nTRAINING_STEP: loss = 77.09478759765625\nTRAINING_STEP: loss = 89.95816040039062\nTRAINING_STEP: loss = 91.39033508300781\nTRAINING_STEP: loss = 77.94834899902344\nTRAINING_STEP: loss = 73.30490112304688\nTRAINING_STEP: loss = 87.18951416015625\nTRAINING_STEP: loss = 72.8819580078125\nTRAINING_STEP: loss = 73.95675659179688\nTRAINING_STEP: loss = 73.77081298828125\nTRAINING_STEP: loss = 73.48324584960938\nTRAINING_STEP: loss = 74.54835510253906\nTRAINING_STEP: loss = 73.28018188476562\nTRAINING_STEP: loss = 76.7403335571289\nTRAINING_STEP: loss = 69.00694274902344\nTRAINING_STEP: loss = 73.1611328125\nTRAINING_STEP: loss = 75.97850799560547\nTRAINING_STEP: loss = 72.59520721435547\nTRAINING_STEP: loss = 74.96592712402344\nTRAINING_STEP: loss = 77.71419525146484\nTRAINING_STEP: loss = 79.86978149414062\nTRAINING_STEP: loss = 79.79158020019531\nTRAINING_STEP: loss = 71.4830093383789\nTRAINING_STEP: loss = 74.93737030029297\nTRAINING_STEP: loss = 73.23310089111328\nTRAINING_STEP: loss = 73.49959564208984\nTRAINING_STEP: loss = 72.96727752685547\nTRAINING_STEP: loss = 73.96713256835938\nTRAINING_STEP: loss = 84.97091674804688\nTRAINING_STEP: loss = 85.80345153808594\nTRAINING_STEP: loss = 73.03201293945312\nTRAINING_STEP: loss = 76.64604187011719\nTRAINING_STEP: loss = 79.98182678222656\nTRAINING_STEP: loss = 85.7503433227539\nTRAINING_STEP: loss = 87.25758361816406\nTRAINING_STEP: loss = 78.58087158203125\nTRAINING_STEP: loss = 74.9858627319336\nTRAINING_STEP: loss = 85.98473358154297\nTRAINING_STEP: loss = 95.15032958984375\nTRAINING_STEP: loss = 79.36721801757812\nTRAINING_STEP: loss = 75.61402893066406\nTRAINING_STEP: loss = 79.37506866455078\nTRAINING_STEP: loss = 90.4707260131836\nTRAINING_STEP: loss = 82.89766693115234\nTRAINING_STEP: loss = 77.8468017578125\nTRAINING_STEP: loss = 77.46874237060547\nTRAINING_STEP: loss = 86.64045715332031\nTRAINING_STEP: loss = 89.22735595703125\nTRAINING_STEP: loss = 84.47980499267578\nTRAINING_STEP: loss = 74.56391906738281\nTRAINING_STEP: loss = 79.93521118164062\nTRAINING_STEP: loss = 90.45020294189453\nTRAINING_STEP: loss = 98.48588562011719\nTRAINING_STEP: loss = 80.54671478271484\nTRAINING_STEP: loss = 69.74359130859375\nTRAINING_STEP: loss = 82.98223114013672\nTRAINING_STEP: loss = 88.40242004394531\nTRAINING_STEP: loss = 78.09437561035156\nTRAINING_STEP: loss = 78.73131561279297\nTRAINING_STEP: loss = 87.95553588867188\nTRAINING_STEP: loss = 100.51749420166016\nTRAINING_STEP: loss = 95.29829406738281\nTRAINING_STEP: loss = 79.45703125\nTRAINING_STEP: loss = 75.91776275634766\nTRAINING_STEP: loss = 84.36178588867188\nTRAINING_STEP: loss = 89.16236114501953\nTRAINING_STEP: loss = 81.8988037109375\nTRAINING_STEP: loss = 77.19503784179688\nTRAINING_STEP: loss = 78.19429016113281\nTRAINING_STEP: loss = 81.92007446289062\nTRAINING_STEP: loss = 78.05316925048828\nTRAINING_STEP: loss = 76.17011260986328\nTRAINING_STEP: loss = 78.29199981689453\nTRAINING_STEP: loss = 76.45503234863281\nTRAINING_STEP: loss = 80.83653259277344\nTRAINING_STEP: loss = 73.12602233886719\nTRAINING_STEP: loss = 76.60289001464844\nTRAINING_STEP: loss = 75.86878967285156\nTRAINING_STEP: loss = 73.95103454589844\nTRAINING_STEP: loss = 74.00625610351562\nTRAINING_STEP: loss = 76.07936096191406\nTRAINING_STEP: loss = 76.64137268066406\nTRAINING_STEP: loss = 83.91207885742188\nTRAINING_STEP: loss = 72.52732849121094\nTRAINING_STEP: loss = 73.75181579589844\nTRAINING_STEP: loss = 79.9590835571289\nTRAINING_STEP: loss = 78.75133514404297\nTRAINING_STEP: loss = 78.19727325439453\nTRAINING_STEP: loss = 77.5440444946289\nTRAINING_STEP: loss = 88.47752380371094\nTRAINING_STEP: loss = 107.63671112060547\nTRAINING_STEP: loss = 107.09938049316406\nTRAINING_STEP: loss = 82.17664337158203\nTRAINING_STEP: loss = 71.046875\nTRAINING_STEP: loss = 84.74309539794922\nTRAINING_STEP: loss = 89.67742919921875\nTRAINING_STEP: loss = 82.12939453125\nTRAINING_STEP: loss = 76.46693420410156\nTRAINING_STEP: loss = 73.74920654296875\nTRAINING_STEP: loss = 79.31182861328125\nTRAINING_STEP: loss = 79.81875610351562\nTRAINING_STEP: loss = 78.11288452148438\nTRAINING_STEP: loss = 73.44664001464844\nTRAINING_STEP: loss = 71.49523162841797\nTRAINING_STEP: loss = 82.00709533691406\nTRAINING_STEP: loss = 80.59506225585938\nTRAINING_STEP: loss = 76.04039001464844\nTRAINING_STEP: loss = 72.8016357421875\nTRAINING_STEP: loss = 79.9407958984375\nTRAINING_STEP: loss = 77.01087951660156\nTRAINING_STEP: loss = 77.60835266113281\nTRAINING_STEP: loss = 73.60935974121094\nTRAINING_STEP: loss = 80.42243194580078\nTRAINING_STEP: loss = 85.88949584960938\nTRAINING_STEP: loss = 82.55259704589844\nTRAINING_STEP: loss = 75.43199157714844\nTRAINING_STEP: loss = 71.08694458007812\nTRAINING_STEP: loss = 78.84844970703125\nTRAINING_STEP: loss = 81.21855163574219\nTRAINING_STEP: loss = 83.55677032470703\nTRAINING_STEP: loss = 81.06903076171875\nTRAINING_STEP: loss = 76.70329284667969\nTRAINING_STEP: loss = 72.97552490234375\nTRAINING_STEP: loss = 78.28289794921875\nTRAINING_STEP: loss = 76.322265625\nTRAINING_STEP: loss = 73.03062438964844\nTRAINING_STEP: loss = 74.65859985351562\nTRAINING_STEP: loss = 76.55308532714844\nTRAINING_STEP: loss = 75.59614562988281\nTRAINING_STEP: loss = 94.39897155761719\nTRAINING_STEP: loss = 89.09709167480469\nTRAINING_STEP: loss = 73.47476959228516\nTRAINING_STEP: loss = 80.01726531982422\nTRAINING_STEP: loss = 90.72711181640625\nTRAINING_STEP: loss = 85.79476928710938\nTRAINING_STEP: loss = 77.13236999511719\nTRAINING_STEP: loss = 81.01179504394531\nTRAINING_STEP: loss = 77.65365600585938\nTRAINING_STEP: loss = 83.46705627441406\nTRAINING_STEP: loss = 71.73872375488281\nTRAINING_STEP: loss = 68.76860046386719\nTRAINING_STEP: loss = 83.6905517578125\nTRAINING_STEP: loss = 83.65245056152344\nTRAINING_STEP: loss = 79.96465301513672\nTRAINING_STEP: loss = 73.89225769042969\nTRAINING_STEP: loss = 76.0273208618164\nTRAINING_STEP: loss = 79.5155029296875\nTRAINING_STEP: loss = 84.83782958984375\nTRAINING_STEP: loss = 95.7064208984375\nTRAINING_STEP: loss = 85.51214599609375\nTRAINING_STEP: loss = 75.33413696289062\nTRAINING_STEP: loss = 78.38754272460938\nTRAINING_STEP: loss = 89.50043487548828\nTRAINING_STEP: loss = 84.03643798828125\nTRAINING_STEP: loss = 76.81256103515625\nTRAINING_STEP: loss = 79.12950134277344\nTRAINING_STEP: loss = 83.6556396484375\nTRAINING_STEP: loss = 76.7105712890625\n","output_type":"stream"}]},{"cell_type":"code","source":"#save weights\ntorch.save(model.state_dict(), 'bart_model.pt')","metadata":{"execution":{"iopub.status.busy":"2024-01-09T15:51:49.424443Z","iopub.execute_input":"2024-01-09T15:51:49.425028Z","iopub.status.idle":"2024-01-09T15:51:50.908043Z","shell.execute_reply.started":"2024-01-09T15:51:49.424980Z","shell.execute_reply":"2024-01-09T15:51:50.907024Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model.to(device)\n\n# Test the model\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n\ndef decimal_to_hexadecimal(decimal):    \n    hex_digits = \"0123456789abcdefghilmnopqrstuvzppppppppppppppppppppp\"\n    return hex_digits[decimal]\n\nfor text, gold in val_loader:\n\n    prediction = model.forward(text[0]).tolist()[0]\n\n    #round every elem in prediction\n    prediction = [round(x) for x in prediction]\n\n    #set to 0 every negative elem in prediction\n    prediction = [max(0, x) for x in prediction]\n\n    #convert every 0 to 0, 1 to 1, ... , 15 to f\n    hex_prediction = [decimal_to_hexadecimal(x) for x in prediction]\n\n    hex_gold = [decimal_to_hexadecimal(x) for x in gold[0].tolist()]\n\n    #convert to string\n    hex_prediction = ''.join(hex_prediction)\n    hex_gold = ''.join(hex_gold)\n\n    print(f\"Prediction: {hex_prediction}\")\n    print(f\"Gold: {hex_gold}\")\n\n\n\n    \n    break","metadata":{"execution":{"iopub.status.busy":"2024-01-09T15:51:50.909682Z","iopub.execute_input":"2024-01-09T15:51:50.910325Z","iopub.status.idle":"2024-01-09T15:51:51.473453Z","shell.execute_reply.started":"2024-01-09T15:51:50.910297Z","shell.execute_reply":"2024-01-09T15:51:51.472113Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Prediction: 9c0edbcd00785d8h0bee0g4a7aigepefedi8ci5014d0gl00hchggqfoc00ee07n0n0eqh0g00hfo0rroppppq000or0pqqoq0s00opp0rqprp0pqti00p0q00oqr00rpqq0qop0qnoqprqosp0nqq00pprqospp00usrnqtrpr0soqq0nrmn00n0pp0rqoonolprqo0000qr0pr0s00srqqp0o00qms00rq00pp0p0qrqqsorq00qro0lprnq0s\nGold: 789cf3cc53c8ad54c84b2d4b2dd24dcd4bc9cc4b57282c4d2d2e01006b3a08f2mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n","output_type":"stream"}]}]}