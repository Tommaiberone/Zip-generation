{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch pandas lightning trl\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 999\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Custom Dataset\n",
    "class TextHexDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['text']\n",
    "\n",
    "        # Convert hex into tensor\n",
    "        hex_data = self.dataframe.iloc[idx]['deflate_hex']\n",
    "        hex_data = [int(x, 16) for x in hex_data]\n",
    "        \n",
    "        #pad to reach 512\n",
    "        padded_hex_data = hex_data + [0] * (512 - len(hex_data))\n",
    "        tensor_hex_data = torch.tensor(padded_hex_data)\n",
    "        return text, tensor_hex_data\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../../Datasets/new_dataset_deflate.csv')\n",
    "\n",
    "# Create datasets\n",
    "dataset = TextHexDataset(df)\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders with batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION_STEP: loss = 29.07815170288086\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]VALIDATION_STEP: loss = 28.568357467651367\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/625 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "class TransformerModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "        self.transformer = BartModel.from_pretrained('facebook/bart-base')\n",
    "\n",
    "        #set the transformer padding character to 0\n",
    "        self.transformer.config.pad_token_id = 0\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128 * 768, 512)\n",
    "\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        # Tokenize the text\n",
    "        input_ids = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        attention_mask = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).attention_mask.to(device)\n",
    "\n",
    "        # Pad input_ids and attention_mask to a fixed length of 512\n",
    "        padded_input_ids = F.pad(input_ids, (0, 128 - input_ids.shape[1]), 'constant', 0)\n",
    "        padded_attention_mask = F.pad(attention_mask, (0, 128 - attention_mask.shape[1]), 'constant', 0)\n",
    "\n",
    "        # Ensure padding is on the device\n",
    "        padded_input_ids = padded_input_ids.to(device)\n",
    "        padded_attention_mask = padded_attention_mask.to(device)\n",
    "\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: padded_input_ids.shape = {padded_input_ids.shape}\")\n",
    "            print(f\"FORWARD: padded_attention_mask.shape = {padded_attention_mask.shape}\")\n",
    "\n",
    "        # Pass tokenized and padded text through the transformer\n",
    "        transformer_output = self.transformer(input_ids=padded_input_ids, attention_mask=padded_attention_mask).last_hidden_state\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: transformer_output.shape = {transformer_output.shape}\")\n",
    "\n",
    "        # Pooling over the sequence dimension\n",
    "        flattened_output = self.flatten(transformer_output)\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: flattened_output.shape = {flattened_output.shape}\")\n",
    "\n",
    "        # Apply the linear layer\n",
    "        final_output = self.linear(flattened_output)\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: final_output.shape = {final_output.shape}\")\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text, hex_data = batch\n",
    "\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP: text = {text}\")\n",
    "            print(f\"TRAINING_STEP: hex_data.shape = {hex_data.shape}\")\n",
    "\n",
    "        # Pass the text through the transformer\n",
    "        transformer_output = self.forward(text)\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP: transformer_output.shape = {transformer_output.shape}\")\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.loss(transformer_output, hex_data.float())\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP: loss = {loss}\")\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text, hex_data = batch\n",
    "        if DEBUG:\n",
    "            print(f\"VALIDATION_STEP: text = {text}\")\n",
    "            print(f\"VALIDATION_STEP: hex_data = {hex_data}\")\n",
    "        \n",
    "        # Pass the text through the transformer\n",
    "        transformer_output = self.forward(text)\n",
    "        if DEBUG:\n",
    "            print(f\"VALIDATION_STEP: transformer_output.shape = {transformer_output.shape}\")\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.loss(transformer_output, hex_data.float())\n",
    "        print(f\"VALIDATION_STEP: loss = {loss}\")\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerModel()\n",
    "trainer = pl.Trainer(max_epochs=1, enable_checkpointing=False, logger=False)\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: In my never-ending quest to see as many quality movies as possible in my lifetime, i stumbled upon this film\n",
      "Hex: 789c1dccd10d83301004d156b600520865386281957c67e0ce48ee1e92df37d2cc0e1b70debc3ef445bee1ec8c4436048912b0e2e3c5529503d66e317e7cb4087d2ba1ffa26a65ca384188ecf69605fd688edc155855ed016233278f\n",
      "Tokenized prediction: [0, 39297, 460, 6470, 9892, 8780, 7797, 7377, 6276, 7823, 7703, 6944, 6500, 8512, 6993, 7050, 7429, 6982, 6458, 7754, 6914, 7126, 8348, 8155, 7893, 7209, 6558, 7590, 6840, 7220, 7655, 7373, 6613, 7786, 6738, 7538, 6866, 6614, 6937, 7844, 6657, 7182, 7320, 6401, 6481, 7131, 7669, 7396, 7202, 7091, 6956, 7255, 7199, 6681, 6999, 7308, 6854, 7439, 6709, 6317, 6906, 6469, 7593, 7064, 6949, 7198, 6692, 6604, 7421, 6928, 6911, 6680, 6538, 6886, 7005, 6648, 6939, 7094, 7154, 7163, 7619, 5987, 6663, 6507, 6794, 7669, 7417, 6116, 6683, 6309, 6296, 7166, 6588, 6535, 6057, 7037, 6337, 6129, 7532, 5780, 5660, 5873, 6108, 5549, 5243, 4491, 4688, 4840, 4397, 4071, 3607, 3893, 3003, 3395, 3162, 2457, 2432, 2358, 2021, 2335, 1838, 1601, 1318, 1299, 1043, 1053, 631, 959]\n",
      "Tokenized gold: [0, 39413, 438, 134, 417, 7309, 417, 698, 417, 39134, 2663, 37114, 417, 27915, 428, 4697, 245, 26919, 3506, 34109, 2517, 1646, 4390, 438, 4111, 242, 288, 1755, 3818, 1942, 134, 242, 6617, 36807, 3272, 417, 176, 7309, 288, 242, 134, 428, 3083, 25607, 438, 246, 4550, 34964, 14650, 134, 3204, 398, 438, 3305, 14586, 37900, 1092, 428, 288, 242, 176, 242, 246, 438, 3118, 2890, 38071, 417, 4280, 242, 32072, 242, 406, 38133, 1749, 5677, 417, 176, 3178, 134, 3145, 102, 2481, 102, 3506, 3245, 35324, 28871, 3204, 506, 4563, 34764, 37379, 37199, 196, 438, 996, 4432, 3118, 196, 36192, 29291, 30442, 506, 2]\n",
      "Decoded prediction: <s> psychic always Susan heroin tender principles telephonegarids tenureken accessible dig yard chest Arkansas Environmental duo hearts prosecution Hunter choosingwho smoking hanging Intelligenceule Terry refugee Additional techniques soil Troy shipping Along conviction dropping Danny killer 74 tri Belgium guidelines Organization KapaghizesBR respectivealia constitutionNot equally breach silence ramp Devpa hundred tragedy Jake suppliers TrudeauNSump Federation Sri somehowfriendly Independent theory holes discrimination readsmb Laura preparation rarely helpful fre700 stance Mars celebrityagh 700 speculation perfectly Mack inform governancedu Chiefs witnessesking headlines cable priorities print gatheringpo routine 6374 fallenAn bomb); shareholders leaves initiative expand famous contributedhen und fiscal committed dog Instagram letter quality feltac Day thing however\n",
      "Decoded gold: <s>789c1dccd10d83301004d156b600520865386281957c67e0ce48ee1e92df37d2cc0e1b70debc3ef445bee1ec8c4436048912b0e2e3c5529503d66e317e7cb4087d2ba1ffa26a65ca384188ecf69605fd688edc155855ed016233278f</s>\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "# Test the model\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "for text, hex in val_loader:\n",
    "\n",
    "    print(f\"Text: {text[0]}\")\n",
    "    print(f\"Hex: {hex[0]}\")\n",
    "\n",
    "    prediction = model.forward(text[0]).tolist()[0]\n",
    "    prediction = [0 if x < 0 else round(x) for x in prediction]\n",
    "    print(f\"Tokenized prediction: {prediction}\")\n",
    "    \n",
    "    #tokenize gold[0]\n",
    "    encoded_gold = tokenizer(hex[0], return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "    print(f\"Tokenized gold: {encoded_gold[0].tolist()}\")\n",
    "\n",
    "    decoded_prediction = tokenizer.decode(prediction)\n",
    "    print(f\"Decoded prediction: {decoded_prediction}\")\n",
    "\n",
    "    decoded_gold = tokenizer.decode(encoded_gold[0].tolist())\n",
    "    print(f\"Decoded gold: {decoded_gold}\")\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
