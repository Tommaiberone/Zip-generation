{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-09T15:47:54.658706Z","iopub.status.busy":"2024-01-09T15:47:54.657745Z","iopub.status.idle":"2024-01-09T15:48:07.611195Z","shell.execute_reply":"2024-01-09T15:48:07.609879Z","shell.execute_reply.started":"2024-01-09T15:47:54.658670Z"},"trusted":true},"outputs":[],"source":["%%capture\n","%pip install torch pandas lightning trl\n","\n","import torch\n","from torch import nn\n","import pytorch_lightning as pl\n","from datasets import load_dataset, Dataset, DatasetDict\n","import pandas as pd\n","\n","import numpy as np\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import BartForConditionalGeneration, BartTokenizer"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-09T15:48:07.614270Z","iopub.status.busy":"2024-01-09T15:48:07.613928Z","iopub.status.idle":"2024-01-09T15:48:07.621182Z","shell.execute_reply":"2024-01-09T15:48:07.620230Z","shell.execute_reply.started":"2024-01-09T15:48:07.614242Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda\n"]}],"source":["SEED = 999\n","torch.manual_seed(SEED)\n","\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)\n","\n","#set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')\n","print(\"Device:\", device)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-09T15:48:07.622589Z","iopub.status.busy":"2024-01-09T15:48:07.622322Z","iopub.status.idle":"2024-01-09T15:48:07.767511Z","shell.execute_reply":"2024-01-09T15:48:07.766690Z","shell.execute_reply.started":"2024-01-09T15:48:07.622546Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from torch.utils.data import DataLoader, Dataset\n","import torch\n","import torch.nn.functional as F\n","\n","# Custom Dataset\n","class TextHexDataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.dataframe = dataframe\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        text = self.dataframe.iloc[idx]['text']\n","\n","        # Convert hex into tensor\n","        hex_data = self.dataframe.iloc[idx]['deflate_hex']\n","        hex_data = [int(x, 16) for x in hex_data]\n","        \n","        #pad to reach 256\n","        padded_hex_data = hex_data + [20] * (256 - len(hex_data))\n","        tensor_hex_data = torch.tensor(padded_hex_data)\n","        return text, tensor_hex_data\n","\n","# Load the dataset\n","df = pd.read_csv('../../Datasets/shorthex2hex.csv')\n","\n","# Create datasets\n","dataset = TextHexDataset(df)\n","\n","# Split the dataset\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","# DataLoaders with batch_size = 1\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers = 4)\n","val_loader = DataLoader(val_dataset, batch_size=16, num_workers = 4)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-09T15:51:58.656928Z","iopub.status.busy":"2024-01-09T15:51:58.656533Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"]}],"source":["import pytorch_lightning as pl\n","from transformers import BartTokenizer, BartModel\n","import torch\n","import torch.nn.functional as F\n","\n","DEBUG = False\n","\n","\n","class TransformerModel(pl.LightningModule):\n","    def __init__(self):\n","        super(TransformerModel, self).__init__()\n","        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n","        self.transformer = BartModel.from_pretrained('facebook/bart-base')\n","\n","        #set the transformer padding character to 20\n","        self.tokenizer.pad_token = self.tokenizer.convert_ids_to_tokens(20)\n","        self.transformer.config.pad_token_id = 20\n","\n","        self.encoder = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(128 * 768, 256),\n","            nn.Dropout(p=0.2)  # Example of adding a dropout\n","            # You can add more layers here\n","        )\n","\n","        self.loss = torch.nn.MSELoss()\n","\n","        self.to(device)\n","\n","    def forward(self, text):\n","\n","        # Tokenize the text with padding\n","        encoding = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n","        input_ids = encoding.input_ids\n","        attention_mask = encoding.attention_mask\n","\n","        # Ensure padding is on the device\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","\n","        # Ensure padding is on the device\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","\n","        if DEBUG:\n","            print(f\"FORWARD: input_ids.shape = {input_ids.shape}\")\n","            print(f\"FORWARD: attention_mask.shape = {attention_mask.shape}\")\n","\n","        # Pass tokenized and padded text through the transformer\n","        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n","        if DEBUG:\n","            print(f\"FORWARD: transformer_output.shape = {transformer_output.shape}\")\n","\n","        # Apply the linear layer\n","        final_output = self.encoder(transformer_output)\n","\n","        if DEBUG:\n","            print(f\"FORWARD: final_output.shape = {final_output.shape}\")\n","\n","        return final_output\n","    \n","    def training_step(self, batch, batch_idx):\n","        text, hex_data = batch\n","\n","        if DEBUG:\n","            print(f\"TRAINING_STEP: text = {text}\")\n","            print(f\"TRAINING_STEP: hex_data.shape = {hex_data.shape}\")\n","\n","        # Pass the text through the transformer\n","        transformer_output = self.forward(text)\n","        if DEBUG:\n","            print(f\"TRAINING_STEP: transformer_output.shape = {transformer_output.shape}\")\n","\n","        # Calculate the loss\n","        loss = self.loss(transformer_output, hex_data.float())\n","        print(f\"TRAINING_STEP: loss = {loss}\")\n","\n","        return loss\n","    \n","    def validation_step(self, batch, batch_idx):\n","        text, hex_data = batch\n","        if DEBUG:\n","            print(f\"VALIDATION_STEP: text = {text}\")\n","            print(f\"VALIDATION_STEP: hex_data = {hex_data}\")\n","        \n","        # Pass the text through the transformer\n","        transformer_output = self.forward(text)\n","        if DEBUG:\n","            print(f\"VALIDATION_STEP: transformer_output.shape = {transformer_output.shape}\")\n","\n","        # Calculate the loss\n","        loss = self.loss(transformer_output, hex_data.float())\n","        print(f\"VALIDATION_STEP: loss = {loss}\")\n","\n","        return loss\n","    \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=1e-5)\n","\n","\n","\n","model = TransformerModel()\n","\n","trainer = pl.Trainer(max_epochs=5, enable_checkpointing=False, logger=False)\n","trainer.fit(model, train_loader, val_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-09T15:51:49.425028Z","iopub.status.busy":"2024-01-09T15:51:49.424443Z","iopub.status.idle":"2024-01-09T15:51:50.908043Z","shell.execute_reply":"2024-01-09T15:51:50.907024Z","shell.execute_reply.started":"2024-01-09T15:51:49.424980Z"},"trusted":true},"outputs":[],"source":["#save weights\n","torch.save(model.state_dict(), 'bart_model.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-09T15:51:50.910325Z","iopub.status.busy":"2024-01-09T15:51:50.909682Z","iopub.status.idle":"2024-01-09T15:51:51.473453Z","shell.execute_reply":"2024-01-09T15:51:51.472113Z","shell.execute_reply.started":"2024-01-09T15:51:50.910297Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Prediction: 9c0edbcd00785d8h0bee0g4a7aigepefedi8ci5014d0gl00hchggqfoc00ee07n0n0eqh0g00hfo0rroppppq000or0pqqoq0s00opp0rqprp0pqti00p0q00oqr00rpqq0qop0qnoqprqosp0nqq00pprqospp00usrnqtrpr0soqq0nrmn00n0pp0rqoonolprqo0000qr0pr0s00srqqp0o00qms00rq00pp0p0qrqqsorq00qro0lprnq0s\n","Gold: 789cf3cc53c8ad54c84b2d4b2dd24dcd4bc9cc4b57282c4d2d2e01006b3a08f2mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n"]}],"source":["model.to(device)\n","\n","# Test the model\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n","\n","def decimal_to_hexadecimal(decimal):    \n","    hex_digits = \"0123456789abcdefghilmnopqrstuvzppppppppppppppppppppp\"\n","    return hex_digits[decimal]\n","\n","hex_predictions = []\n","hex_golds = []\n","\n","for text, gold in val_loader:\n","\n","    prediction = model.forward(text[0]).tolist()[0]\n","\n","    #round every elem in prediction\n","    prediction = [round(x) for x in prediction]\n","\n","    #set to 0 every negative elem in prediction\n","    prediction = [max(0, x) for x in prediction]\n","\n","    #convert every 0 to 0, 1 to 1, ... , 15 to f\n","    hex_prediction = [decimal_to_hexadecimal(x) for x in prediction]\n","\n","    hex_gold = [decimal_to_hexadecimal(x) for x in gold[0].tolist()]\n","\n","    #convert to string\n","    hex_prediction = ''.join(hex_prediction)\n","    hex_gold = ''.join(hex_gold)\n","\n","    print(f\"Prediction: {hex_prediction}\")\n","    print(f\"Gold: {hex_gold}\")\n","\n","    hex_predictions.append(hex_prediction)\n","    hex_golds.append(hex_gold)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.metrics.distance import edit_distance\n","\n","assert len(hex_predictions) == len(hex_golds)\n","\n","scores = []\n","pred_lenghts = []\n","gold_lenghts = []\n","\n","for i in range(len(hex_predictions)):\n","    pred = hex_predictions[i]\n","    gold = hex_golds[i]\n","    scores.append(edit_distance(pred, gold))\n","    pred_lenghts.append(len(pred))\n","    gold_lenghts.append(len(gold))\n","    \n","print(f\"Average prediction lenght is {np.mean(pred_lenghts)}\")\n","print(f\"Average gold lenght is {np.mean(gold_lenghts)}\")\n","print(f\"Average distance is {np.mean(scores)}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4281201,"sourceId":7368959,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
