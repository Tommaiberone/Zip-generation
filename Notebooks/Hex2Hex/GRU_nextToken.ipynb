{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkKtT4GK-7y1"
      },
      "source": [
        "# Imports and installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "woNI1XG6H-Bb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install lightning datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBg7g-c1A7kR",
        "outputId": "bbe0938a-0ed3-46ce-994a-068e44c1d387"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 999\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import lightning as L\n",
        "import random\n",
        "\n",
        "SEED = 999\n",
        "BATCH_SIZE = 32\n",
        "torch.manual_seed(SEED)\n",
        "L.seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afxIsaS-8AhR",
        "outputId": "a1e4687d-f5ba-46b6-b32e-9a25e7350675"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwbZbYZN_05Z"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "f-sQt-E_Arl2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../../Datasets/shorthex2hex.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aT-D7npnp_3p",
        "outputId": "bcc0e7d5-cc0d-423b-fc54-0ddd1983ef14"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_hex</th>\n",
              "      <th>deflate_hex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other</td>\n",
              "      <td>4f6e65206f6620746865206f74686572</td>\n",
              "      <td>789cf3cf4b55c84f5328c9005240a208002eb405bb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production.</td>\n",
              "      <td>4120776f6e64657266756c206c6974746c652070726f64...</td>\n",
              "      <td>789c735428cfcf4b492d4a2bcd51c8c92c29c949552828...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was</td>\n",
              "      <td>492074686f75676874207468697320776173</td>\n",
              "      <td>789cf35428c9c82f4dcf2801d299c50ae589c5003dea06b0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family</td>\n",
              "      <td>4261736963616c6c79207468657265277320612066616d...</td>\n",
              "      <td>789c734a2cce4c4eccc9a95428c9482d4a552f56485448...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in</td>\n",
              "      <td>506574746572204d6174746569277320224c6f766520696e</td>\n",
              "      <td>789c0b482d29492d52f04d045299eac50a4a3ef965a90a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             text  \\\n",
              "0                One of the other   \n",
              "1  A wonderful little production.   \n",
              "2              I thought this was   \n",
              "3      Basically there's a family   \n",
              "4        Petter Mattei's \"Love in   \n",
              "\n",
              "                                            text_hex  \\\n",
              "0                   4f6e65206f6620746865206f74686572   \n",
              "1  4120776f6e64657266756c206c6974746c652070726f64...   \n",
              "2               492074686f75676874207468697320776173   \n",
              "3  4261736963616c6c79207468657265277320612066616d...   \n",
              "4   506574746572204d6174746569277320224c6f766520696e   \n",
              "\n",
              "                                         deflate_hex  \n",
              "0         789cf3cf4b55c84f5328c9005240a208002eb405bb  \n",
              "1  789c735428cfcf4b492d4a2bcd51c8c92c29c949552828...  \n",
              "2   789cf35428c9c82f4dcf2801d299c50ae589c5003dea06b0  \n",
              "3  789c734a2cce4c4eccc9a95428c9482d4a552f56485448...  \n",
              "4  789c0b482d29492d52f04d045299eac50a4a3ef965a90a...  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "A-FjhyF1U0mc"
      },
      "outputs": [],
      "source": [
        "df = df[:10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVUV1WDuvi7G"
      },
      "source": [
        "Instead of using the standard \\<EOS> and \\<SOS> tags we're using the letter S and E since they are not present in the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0RT1ITUEvGqg"
      },
      "outputs": [],
      "source": [
        "df['text_hex'] = 'S' + df['text_hex'] + 'E'\n",
        "df['deflate_hex'] = 'S' + df['deflate_hex'] + 'E'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE9OmLvsIdlf",
        "outputId": "4167d401-850a-439c-c64e-7a188b80ae70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'text_hex', 'deflate_hex'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['text', 'text_hex', 'deflate_hex'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'text_hex', 'deflate_hex'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds = Dataset.from_pandas(df)\n",
        "ds_train_test = ds.train_test_split(test_size=0.2, seed=SEED)\n",
        "ds_test_dev = ds_train_test['test'].train_test_split(test_size=0.5, seed=SEED)\n",
        "ds_splits = DatasetDict({\n",
        "    'train': ds_train_test['train'],\n",
        "    'valid': ds_test_dev['train'],\n",
        "    'test': ds_test_dev['test']\n",
        "})\n",
        "\n",
        "ds_splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peMz7QU3qja8"
      },
      "source": [
        "# Tokenizzare in caratteri singoli o in sequenze di caratteri?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9OJyrpZqaoA",
        "outputId": "8e69d1d5-be54-4af2-b106-84350d2e189b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'I really enjoyed this',\n",
              " 'text_hex': 'S49207265616c6c7920656e6a6f7965642074686973E',\n",
              " 'deflate_hex': 'S789cf354284a4dccc9a95448cdcbcaaf4c4d5128c9c82c0600533607d9E'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_splits['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBdD8nIBrL30"
      },
      "source": [
        "## Data tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KJRiqX3zxmHR"
      },
      "outputs": [],
      "source": [
        "token2id = {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6, \"7\": 7, \"8\": 8, \"9\": 9, \"a\": 10, \"b\": 11, \"c\": 12, \"d\": 13, \"e\": 14, \"f\": 15, \"P\":16, \"S\": 17, \"E\":18 }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4dMwVcvySQQ",
        "outputId": "5eb93c58-c71f-413a-dd2b-2e99d459e3a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '0',\n",
              " 1: '1',\n",
              " 2: '2',\n",
              " 3: '3',\n",
              " 4: '4',\n",
              " 5: '5',\n",
              " 6: '6',\n",
              " 7: '7',\n",
              " 8: '8',\n",
              " 9: '9',\n",
              " 10: 'a',\n",
              " 11: 'b',\n",
              " 12: 'c',\n",
              " 13: 'd',\n",
              " 14: 'e',\n",
              " 15: 'f',\n",
              " 16: 'P',\n",
              " 17: 'S',\n",
              " 18: 'E'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_id2token_vocab(token_to_id):\n",
        "    id2token = {}\n",
        "    for token, id in token_to_id.items():\n",
        "        id2token[id] = token\n",
        "\n",
        "    return id2token\n",
        "\n",
        "id2token = create_id2token_vocab(token2id)\n",
        "id2token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VF82QijJt4nw"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "\n",
        "  def pad_sequences(sequences, maxlen, value=token2id['P']):\n",
        "    padded_sequences = []\n",
        "    for sequence in sequences:\n",
        "        padded_sequence = sequence[:maxlen]\n",
        "        padded_sequence.extend([value] * (maxlen - len(padded_sequence)))\n",
        "\n",
        "        padded_sequence = sequence +  [value] * (maxlen - len(sequence))\n",
        "        padded_sequences.append(padded_sequence)\n",
        "\n",
        "    return padded_sequences\n",
        "\n",
        "\n",
        "  texts = [elem['text_hex'] for elem in batch]\n",
        "  encoded_hex = [[token2id[x] for x in hex] for hex in texts]\n",
        "\n",
        "\n",
        "  outputs = [elem['deflate_hex'] for elem in batch]\n",
        "  encoded_outputs = [[token2id[x] for x in hex] for hex in outputs]\n",
        "\n",
        "\n",
        "  maxlen = 0\n",
        "  for seq in encoded_hex:\n",
        "    if len(seq) > maxlen:\n",
        "      maxlen = len(seq)\n",
        "  for seq in encoded_outputs:\n",
        "    if len(seq) > maxlen:\n",
        "      maxlen = len(seq)\n",
        "\n",
        "  padded_encoded_hex = pad_sequences(encoded_hex, maxlen)\n",
        "  padded_encoded_outputs = pad_sequences(encoded_outputs, maxlen)\n",
        "\n",
        "\n",
        "  return {\n",
        "      'inputs': torch.tensor(padded_encoded_hex),\n",
        "      \"outputs\": torch.tensor(padded_encoded_outputs)\n",
        "  }\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdvnGQ8WZ_a6"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "b_yHFnX4aAUM"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_len, embedding_dim, hidden_dim, num_layers, bidirectional, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_len = vocab_len\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_len, embedding_dim, padding_idx=token2id['P'])\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        x = self.embedding(batch)\n",
        "        x = self.dropout(x)\n",
        "        outputs, h = self.gru(x)\n",
        "        return h\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fyoFzpgNrCBs"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_len, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_len, embedding_dim, padding_idx=token2id['P'])\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        x = x.unsqueeze(0)\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        out, h = self.gru(embedding, h)\n",
        "        logits = self.linear(out.squeeze(0))\n",
        "        return logits, h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YNs_PbllLqi"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX3UetO5xAgU",
        "outputId": "bc2f3f9e-21e6-4255-d019-ec4f0ac7503d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | encoder   | Encoder          | 22.8 M\n",
            "1 | decoder   | Decoder          | 22.9 M\n",
            "2 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "45.7 M    Trainable params\n",
            "0         Non-trainable params\n",
            "45.7 M    Total params\n",
            "182.766   Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 34.45it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1:  47%|████▋     | 117/250 [00:42<00:48,  2.76it/s, v_num=4]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
          ]
        }
      ],
      "source": [
        "import pytorch_lightning as pl\n",
        "EPOCHS = 20\n",
        "LR = 1e-3\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 1024\n",
        "NUM_LAYERS = 4\n",
        "DROPOUT = 0.3\n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "# Combined EncoderDecoder model\n",
        "class EncoderDecoder(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = Encoder(len(token2id), EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
        "        self.decoder = Decoder(len(token2id), EMBEDDING_DIM, HIDDEN_DIM, len(token2id), NUM_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=token2id['P'])\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        target_len = source.shape[0]\n",
        "        batch_size = source.shape[1]\n",
        "        target_vocab_size = len(token2id)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        h = self.encoder(source)\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, h = self.decoder(x, h)\n",
        "            outputs[t] = output\n",
        "            pred = output.argmax(1)\n",
        "            x = target[t] if random.random() < teacher_forcing_ratio else pred\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, targets = batch['inputs'], batch['outputs']\n",
        "        output = self(inputs, targets)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        targets = targets[1:].view(-1)\n",
        "        loss = self.criterion(output, targets)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, targets = batch['inputs'], batch['outputs']\n",
        "        output = self(inputs, targets)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        targets = targets[1:].view(-1)\n",
        "        loss = self.criterion(output, targets)\n",
        "        self.log('val_loss', loss)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        inputs, targets = batch['inputs'], batch['outputs']\n",
        "        output = self(inputs, targets)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        targets = targets[1:].view(-1)\n",
        "        loss = self.criterion(output, targets)\n",
        "        self.log('test_loss', loss)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=LR)\n",
        "    \n",
        "\n",
        "class HexDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_dataset, val_dataset, test_dataset):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize the data module and model\n",
        "data_module = HexDataModule(ds_splits['train'], ds_splits['valid'], ds_splits['test'])\n",
        "model = EncoderDecoder()\n",
        "\n",
        "# Train the model\n",
        "trainer = pl.Trainer(max_epochs=EPOCHS)\n",
        "trainer.fit(model, datamodule=data_module)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bXTjWSC1YZ0",
        "outputId": "2daae1bd-319a-48de-adce-7c8cd96bdf67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outputs = [17, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "# Create the test dataloader\n",
        "test_dataloader = data_module.test_dataloader()\n",
        "\n",
        "# Now proceed with the test\n",
        "with torch.no_grad():\n",
        "    # Get a batch from the test dataloader\n",
        "    batch = next(iter(test_dataloader))\n",
        "    h = model.encoder(batch['inputs'][0].unsqueeze(1).to('cpu'))\n",
        "\n",
        "    outputs = [token2id[\"S\"]]\n",
        "    for i in range(60):\n",
        "        prev = torch.LongTensor([outputs[-1]]).to('cpu')\n",
        "        out, h = model.decoder(prev, h)  # No cell state 'c' needed for GRU\n",
        "        pred = out.argmax(1).item()\n",
        "\n",
        "        outputs.append(pred)\n",
        "        if pred == token2id['E']:\n",
        "            break\n",
        "\n",
        "    print(f\"Outputs = {outputs}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
