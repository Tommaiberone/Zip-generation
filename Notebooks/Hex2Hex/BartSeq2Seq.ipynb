{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-11T10:10:31.691653Z","iopub.status.busy":"2024-01-11T10:10:31.691242Z","iopub.status.idle":"2024-01-11T10:12:21.565892Z","shell.execute_reply":"2024-01-11T10:12:21.564583Z","shell.execute_reply.started":"2024-01-11T10:10:31.691619Z"},"trusted":true},"outputs":[],"source":["%%capture\n","! pip install datasets\n","! pip install transformers -U\n","! pip install accelerate -U\n","! pip install evaluate\n","! pip install bleu\n","! pip install python-Levenshtein\n","! pip install wandb"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T10:14:26.747376Z","iopub.status.busy":"2024-01-11T10:14:26.746733Z","iopub.status.idle":"2024-01-11T10:14:56.366224Z","shell.execute_reply":"2024-01-11T10:14:56.365322Z","shell.execute_reply.started":"2024-01-11T10:14:26.747345Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["from typing import Dict, List, Tuple\n","from dataclasses import dataclass\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import pandas as pd\n","from datasets import load_dataset, Dataset, DatasetDict\n","\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","\n","SEED = 999\n","torch.manual_seed(SEED)\n","\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["LOG = False\n","\n","if LOG:\n","  import wandb\n","  wandb.login()\n","\n","  import os\n","  os.environ[\"WANDB_PROJECT\"] = \"Seq2SeqZip\""]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T10:14:56.369164Z","iopub.status.busy":"2024-01-11T10:14:56.368131Z","iopub.status.idle":"2024-01-11T10:14:56.643206Z","shell.execute_reply":"2024-01-11T10:14:56.642240Z","shell.execute_reply.started":"2024-01-11T10:14:56.369127Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                             text  \\\n","0                One of the other   \n","1  A wonderful little production.   \n","2              I thought this was   \n","3      Basically there's a family   \n","4        Petter Mattei's \"Love in   \n","\n","                                            text_hex  \\\n","0                   4f6e65206f6620746865206f74686572   \n","1  4120776f6e64657266756c206c6974746c652070726f64...   \n","2               492074686f75676874207468697320776173   \n","3  4261736963616c6c79207468657265277320612066616d...   \n","4   506574746572204d6174746569277320224c6f766520696e   \n","\n","                                         deflate_hex  \n","0         789cf3cf4b55c84f5328c9005240a208002eb405bb  \n","1  789c735428cfcf4b492d4a2bcd51c8c92c29c949552828...  \n","2   789cf35428c9c82f4dcf2801d299c50ae589c5003dea06b0  \n","3  789c734a2cce4c4eccc9a95428c9482d4a552f56485448...  \n","4  789c0b482d29492d52f04d045299eac50a4a3ef965a90a...  \n"]}],"source":["df = pd.read_csv('/kaggle/input/hexadecimalzip/shorthex.csv')\n","df = df[:8000]\n","print(df.head())\n","df['deflate_hex'] = [elem + \"</s>\" for elem in df['deflate_hex']]      \n","df['text_hex'] = [elem + \"</s>\" for elem in df['text_hex']]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T10:14:56.644765Z","iopub.status.busy":"2024-01-11T10:14:56.644466Z","iopub.status.idle":"2024-01-11T10:14:56.758853Z","shell.execute_reply":"2024-01-11T10:14:56.757935Z","shell.execute_reply.started":"2024-01-11T10:14:56.644739Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'text_hex', 'deflate_hex'],\n","        num_rows: 6400\n","    })\n","    valid: Dataset({\n","        features: ['text', 'text_hex', 'deflate_hex'],\n","        num_rows: 800\n","    })\n","    test: Dataset({\n","        features: ['text', 'text_hex', 'deflate_hex'],\n","        num_rows: 800\n","    })\n","})\n"]}],"source":["ds = Dataset.from_pandas(df)\n","ds_train_test = ds.train_test_split(test_size=0.2, seed=SEED)\n","ds_test_dev = ds_train_test['test'].train_test_split(test_size=0.5, seed=SEED)\n","ds_splits = DatasetDict({\n","    'train': ds_train_test['train'],\n","    'valid': ds_test_dev['train'],\n","    'test': ds_test_dev['test']\n","})\n","\n","print(ds_splits)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T10:14:56.762181Z","iopub.status.busy":"2024-01-11T10:14:56.761578Z","iopub.status.idle":"2024-01-11T10:14:56.768369Z","shell.execute_reply":"2024-01-11T10:14:56.767447Z","shell.execute_reply.started":"2024-01-11T10:14:56.762147Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'text': 'I just finished watching',\n"," 'text_hex': '49206a7573742066696e6973686564207761746368696e67</s>',\n"," 'deflate_hex': '789cf354c82a2d2e5148cbcccb2cce484d51284f2c49cec8cc4b07006d1d090f</s>'}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["ds_splits['train'][0]"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T10:14:56.770470Z","iopub.status.busy":"2024-01-11T10:14:56.769663Z","iopub.status.idle":"2024-01-11T10:15:02.403827Z","shell.execute_reply":"2024-01-11T10:15:02.402388Z","shell.execute_reply.started":"2024-01-11T10:14:56.770436Z"},"trusted":true},"outputs":[],"source":["model_name = \"facebook/bart-base\"\n","tokenizer = BartTokenizer.from_pretrained(model_name)\n","model = BartForConditionalGeneration.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T10:15:02.405594Z","iopub.status.busy":"2024-01-11T10:15:02.405230Z","iopub.status.idle":"2024-01-11T10:15:02.416521Z","shell.execute_reply":"2024-01-11T10:15:02.415367Z","shell.execute_reply.started":"2024-01-11T10:15:02.405565Z"},"trusted":true},"outputs":[],"source":["@dataclass\n","class DataCollatorSeq2SeqWithPadding:\n","    tokenizer: BartTokenizer\n","\n","    def __call__(self, dataset_elements) -> Dict[str, torch.Tensor]:\n","\n","        # collect the input and output sequences\n","        input_text = [de[\"text_hex\"] for de in dataset_elements]\n","        output_text = [de[\"deflate_hex\"] for de in dataset_elements]\n","\n","        # tokenize both sequences in batch so that it will be much faster!\n","        input_features = self.tokenizer(\n","            input_text,\n","            return_tensors=\"pt\",  # output directly tensors\n","            padding=True, # add the padding on each sequence if needed\n","            truncation=True # If the input sequence is too long, truncate it\n","        )\n","\n","        output_features = self.tokenizer(\n","            output_text,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True\n","        )[\"input_ids\"]  # here we only need the input_ids (output actually)\n","\n","        output_features[output_features==self.tokenizer.pad_token_id] = -100 # cross entropy ignore index\n","\n","        # This is the only parameters we need for the forward pass\n","        # to understand why, take a look to the BartForConditionalGeneration.forward method signature.\n","        batch = {\n","            \"input_ids\": input_features[\"input_ids\"],\n","            \"attention_mask\": input_features[\"attention_mask\"],\n","            \"labels\": output_features,\n","        }\n","\n","        return batch"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T10:15:02.418328Z","iopub.status.busy":"2024-01-11T10:15:02.417920Z","iopub.status.idle":"2024-01-11T10:15:02.519554Z","shell.execute_reply":"2024-01-11T10:15:02.518368Z","shell.execute_reply.started":"2024-01-11T10:15:02.418271Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorSeq2SeqWithPadding(tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["# Trainer"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T10:17:40.732713Z","iopub.status.busy":"2024-01-11T10:17:40.731693Z","iopub.status.idle":"2024-01-11T10:17:40.742977Z","shell.execute_reply":"2024-01-11T10:17:40.742068Z","shell.execute_reply.started":"2024-01-11T10:17:40.732667Z"},"trusted":true},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"temp\",\n","    per_device_train_batch_size=8,\n","    gradient_accumulation_steps=1,\n","    learning_rate=2e-5,\n","    warmup_steps=500,\n","    max_steps=10000,\n","    evaluation_strategy=\"steps\",\n","    fp16=True,\n","    per_device_eval_batch_size=8,\n","    generation_max_length=250,\n","    eval_steps=1000,  # evaluate on the validation every \"eval_steps\"\n","    logging_steps=1000,  # log standard metrics each \"logging_steps\"\n","    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n","    label_names=[\"labels\"],  # same reason as above\n","    predict_with_generate=True,\n","    save_strategy = \"no\"\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T10:17:42.912476Z","iopub.status.busy":"2024-01-11T10:17:42.911714Z","iopub.status.idle":"2024-01-11T10:17:42.918569Z","shell.execute_reply":"2024-01-11T10:17:42.917590Z","shell.execute_reply.started":"2024-01-11T10:17:42.912441Z"},"trusted":true},"outputs":[],"source":["## UNUSED FOR NOW\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","\n","    # decode preds and labels\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    print(f\"Decoded preds = {decoded_preds}\\n\\n\")\n","    print(f\"Decoded labels = {decoded_labels}\")\n","\n","    #result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    return 0"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T10:17:44.400532Z","iopub.status.busy":"2024-01-11T10:17:44.399756Z","iopub.status.idle":"2024-01-11T10:17:44.799213Z","shell.execute_reply":"2024-01-11T10:17:44.798180Z","shell.execute_reply.started":"2024-01-11T10:17:44.400496Z"},"trusted":true},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=ds_splits[\"train\"],\n","    eval_dataset=ds_splits[\"valid\"],\n","    data_collator=data_collator,\n","    #compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T10:17:46.620563Z","iopub.status.busy":"2024-01-11T10:17:46.619489Z","iopub.status.idle":"2024-01-11T11:07:55.278926Z","shell.execute_reply":"2024-01-11T11:07:55.277821Z","shell.execute_reply.started":"2024-01-11T10:17:46.620517Z"},"trusted":true},"outputs":[{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.16.2 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240111_101747-rxst59ai</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/teglia-simone/Seq2SeqZip/runs/rxst59ai' target=\"_blank\">zesty-mountain-1</a></strong> to <a href='https://wandb.ai/teglia-simone/Seq2SeqZip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/teglia-simone/Seq2SeqZip' target=\"_blank\">https://wandb.ai/teglia-simone/Seq2SeqZip</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/teglia-simone/Seq2SeqZip/runs/rxst59ai' target=\"_blank\">https://wandb.ai/teglia-simone/Seq2SeqZip/runs/rxst59ai</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [10000/10000 49:35, Epoch 25/25]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.598000</td>\n","      <td>1.212662</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.127200</td>\n","      <td>0.831910</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.831300</td>\n","      <td>0.713727</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.710700</td>\n","      <td>0.651779</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.635700</td>\n","      <td>0.614619</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.589700</td>\n","      <td>0.597690</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.556100</td>\n","      <td>0.583713</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.533900</td>\n","      <td>0.578531</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.516400</td>\n","      <td>0.571998</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.506900</td>\n","      <td>0.570023</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=10000, training_loss=0.8605920043945312, metrics={'train_runtime': 3008.2586, 'train_samples_per_second': 53.187, 'train_steps_per_second': 3.324, 'total_flos': 3360143295774720.0, 'train_loss': 0.8605920043945312, 'epoch': 25.0})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Save model if necessary"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T11:30:19.865562Z","iopub.status.busy":"2024-01-11T11:30:19.865072Z","iopub.status.idle":"2024-01-11T11:30:21.100206Z","shell.execute_reply":"2024-01-11T11:30:21.099091Z","shell.execute_reply.started":"2024-01-11T11:30:19.865526Z"},"trusted":true},"outputs":[],"source":["trainer.save_model(\"/kaggle/working/bart_model\")"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T11:32:48.885534Z","iopub.status.busy":"2024-01-11T11:32:48.885115Z","iopub.status.idle":"2024-01-11T11:33:22.283290Z","shell.execute_reply":"2024-01-11T11:33:22.281387Z","shell.execute_reply.started":"2024-01-11T11:32:48.885501Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: kaggle/working/bart_model/ (stored 0%)\n","  adding: kaggle/working/bart_model/merges.txt (deflated 53%)\n","  adding: kaggle/working/bart_model/training_args.bin (deflated 49%)\n","  adding: kaggle/working/bart_model/vocab.json (deflated 68%)\n","  adding: kaggle/working/bart_model/tokenizer_config.json (deflated 76%)\n","  adding: kaggle/working/bart_model/special_tokens_map.json (deflated 85%)\n","  adding: kaggle/working/bart_model/generation_config.json (deflated 47%)\n","  adding: kaggle/working/bart_model/config.json (deflated 64%)\n","  adding: kaggle/working/bart_model/model.safetensors (deflated 8%)\n"]}],"source":["!zip -r bart_model.zip /kaggle/working/bart_model"]},{"cell_type":"markdown","metadata":{},"source":["# TEST"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T11:12:17.637206Z","iopub.status.busy":"2024-01-11T11:12:17.636768Z","iopub.status.idle":"2024-01-11T11:12:17.644111Z","shell.execute_reply":"2024-01-11T11:12:17.642964Z","shell.execute_reply.started":"2024-01-11T11:12:17.637174Z"},"trusted":true},"outputs":[],"source":["test_dataloader = torch.utils.data.DataLoader(ds_splits[\"test\"], batch_size=8, collate_fn=data_collator)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T11:14:42.961174Z","iopub.status.busy":"2024-01-11T11:14:42.960763Z","iopub.status.idle":"2024-01-11T11:16:14.937632Z","shell.execute_reply":"2024-01-11T11:16:14.936195Z","shell.execute_reply.started":"2024-01-11T11:14:42.961141Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [01:31<00:00,  1.09it/s]\n"]}],"source":["gold_strings = []\n","predicted_strings = []\n","\n","model.eval()\n","for step, batch in enumerate(tqdm(test_dataloader)):\n","    with torch.cuda.amp.autocast():\n","        with torch.inference_mode():\n","\n","            generated_tokens = (\n","                model.generate(\n","                    input_ids=batch[\"input_ids\"].to(\"cuda\"),\n","                    max_new_tokens=255,\n","                )\n","                .cpu()\n","                .numpy()\n","            )\n","\n","            labels = batch[\"labels\"].cpu().numpy()\n","            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","\n","            # turn subwords ids back into text\n","            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","            #print(\"Gold summary: \", decoded_labels)\n","            #print(\"Predicted summary: \", decoded_preds)\n","            \n","            gold_strings.extend(decoded_labels)\n","            predicted_strings.extend(decoded_preds)\n","            \n","            \n","    del generated_tokens, labels, batch"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T11:25:22.634449Z","iopub.status.busy":"2024-01-11T11:25:22.634016Z","iopub.status.idle":"2024-01-11T11:25:26.476715Z","shell.execute_reply":"2024-01-11T11:25:26.475259Z","shell.execute_reply.started":"2024-01-11T11:25:22.634415Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average prediction lenght is 55.55875\n","Average gold lenght is 55.3275\n","Average distance is 7.305\n"]}],"source":["import nltk\n","from nltk.metrics.distance import edit_distance\n","\n","assert len(predicted_strings) == len(gold_strings)\n","\n","scores = []\n","pred_lenghts = []\n","gold_lenghts = []\n","\n","for i in range(len(predicted_strings)):\n","    pred = predicted_strings[i]\n","    gold = gold_strings[i]\n","    scores.append(edit_distance(pred, gold))\n","    pred_lenghts.append(len(pred))\n","    gold_lenghts.append(len(gold))\n","    \n","print(f\"Average prediction lenght is {np.mean(pred_lenghts)}\")\n","print(f\"Average gold lenght is {np.mean(gold_lenghts)}\")\n","print(f\"Average distance is {np.mean(scores)}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4290346,"sourceId":7382235,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
