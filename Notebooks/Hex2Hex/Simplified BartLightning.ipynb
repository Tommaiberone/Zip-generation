{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7632616,"sourceType":"datasetVersion","datasetId":4447546}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install datasets transformers accelerate evaluate wandb nltk pandas pytorch_lightning","metadata":{"id":"PhAeBJfNJsy-","execution":{"iopub.status.busy":"2024-02-28T15:10:08.971934Z","iopub.execute_input":"2024-02-28T15:10:08.972867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainingArguments\nfrom torch.utils.data import DataLoader\nfrom nltk.metrics.distance import edit_distance\nimport pytorch_lightning as pl\nfrom torch.optim import AdamW\nfrom dataclasses import dataclass\n\n# Set up wandb environment\nos.environ[\"WANDB_PROJECT\"] = \"Seq2SeqZip\"\n\n# Seed for reproducibility\nSEED = 999\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n","metadata":{"id":"hdp0yz7mJsy_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to load and preprocess dataset\ndef load_and_preprocess_data(filepath):\n    df = pd.read_csv(filepath)[:8000]\n    df[['deflate_hex', 'text_hex', 'text']] += \"</s>\"\n    ds = Dataset.from_pandas(df)\n    ds_train_test = ds.train_test_split(test_size=0.2, seed=SEED)\n    ds_test_dev = ds_train_test['test'].train_test_split(test_size=0.5, seed=SEED)\n    return DatasetDict({\n        'train': ds_train_test['train'],\n        'valid': ds_test_dev['train'],\n        'test': ds_test_dev['test']\n    })\n\nds_splits = load_and_preprocess_data('/kaggle/input/full-dataset/randomized_shorthex2hex.csv')","metadata":{"id":"1nFsQ146JszA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass DataCollatorSeq2SeqWithPadding:\n    tokenizer: BartTokenizer\n\n    def __call__(self, dataset_elements):\n        inputs = [x[\"text_hex\"] for x in dataset_elements]\n        outputs = [x[\"deflate_hex\"] for x in dataset_elements]\n        input_features = self.tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n        output_features = self.tokenizer(outputs, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n        output_features[output_features == self.tokenizer.pad_token_id] = -100\n        return {\"input_ids\": input_features[\"input_ids\"], \"attention_mask\": input_features[\"attention_mask\"], \"labels\": output_features}\n","metadata":{"id":"u-PSQfryJszA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MODEL CHOICES: bart-base, bart-large, t5-base\nMODEL = \"bart-base\"\n\nif (MODEL == \"bart-base\"):\n    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n\nelif (MODEL == \"bart-large\"):\n    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n\ndata_collator = DataCollatorSeq2SeqWithPadding(tokenizer)","metadata":{"id":"PGd5_RNtJszA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(preds, labels, tokenizer):\n    # Ensure labels with -100 are replaced by pad_token_id\n    labels = torch.where(labels == -100, tokenizer.pad_token_id, labels)\n\n    # Convert tensors to lists for decoding if they're not already in CPU\n    if torch.is_tensor(preds):\n        preds = preds.detach().cpu().tolist()\n    if torch.is_tensor(labels):\n        labels = labels.detach().cpu().tolist()\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    distances = [edit_distance(p, l) for p, l in zip(decoded_preds, decoded_labels)]\n    avg_distance = np.mean(distances)\n    count_unzippable = distances.count(0)\n\n    return {\"average_edit_distance\": avg_distance, \"count_unzippable\": count_unzippable}\n","metadata":{"id":"QuX0gGj2JszA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nMAX_EPOCHS = 10\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-2\n\nclass Seq2Seq(pl.LightningModule):\n    def __init__(self, tokenizer, model, data_collator):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.model = model\n        self.data_collator = data_collator\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n\n    def training_step(self, batch, batch_idx):\n        outputs = self.forward(**batch)\n        self.log('train_loss', outputs.loss, prog_bar=True, logger=True)\n        return outputs.loss\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self.forward(**batch)\n        self.log('val_loss', outputs.loss, prog_bar=True, logger=True)\n\n        preds = torch.argmax(outputs.logits, dim=-1)\n        metrics = compute_metrics(preds, batch['labels'], self.tokenizer)\n        for key, value in metrics.items():\n            self.log(f'{key}', value, prog_bar=True, logger=True)\n\n        return outputs.loss\n\n    def test_step(self, batch, batch_idx):\n        outputs = self.forward(**batch)\n        self.log('test_loss', outputs.loss, prog_bar=True, logger=True)\n        return outputs.loss\n\n    def configure_optimizers(self):\n        # Directly use learning rate and weight decay values here\n        return AdamW(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\ntrainer = pl.Trainer(\n    precision='16-mixed',\n    max_epochs=MAX_EPOCHS,\n    enable_progress_bar=True,\n)\n\ntrain_dataloader = DataLoader(ds_splits[\"train\"], batch_size=BATCH_SIZE, shuffle = True, collate_fn=data_collator, num_workers = 3)\nvalid_dataloader = DataLoader(ds_splits[\"valid\"], batch_size=BATCH_SIZE, shuffle = False, collate_fn=data_collator, num_workers = 3)\n\nseq2seq_model = Seq2Seq(tokenizer, model, data_collator)\ntrainer.fit(seq2seq_model, train_dataloader, valid_dataloader)\n\ntest_dataloader = DataLoader(ds_splits[\"test\"], batch_size=BATCH_SIZE, shuffle = False, collate_fn=data_collator, num_workers = 3)\ntrainer.test(seq2seq_model, test_dataloader)","metadata":{"id":"mvEo6XxsJszA","trusted":true},"execution_count":null,"outputs":[]}]}