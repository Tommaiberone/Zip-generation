{"cells":[{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T19:49:18.808747Z","iopub.status.busy":"2024-02-28T19:49:18.808354Z","iopub.status.idle":"2024-02-28T19:49:32.156931Z","shell.execute_reply":"2024-02-28T19:49:32.155543Z","shell.execute_reply.started":"2024-02-28T19:49:18.808709Z"},"id":"PhAeBJfNJsy-","trusted":true},"outputs":[],"source":["%%capture\n","!pip install datasets transformers accelerate evaluate wandb nltk pandas pytorch_lightning"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T19:49:32.160029Z","iopub.status.busy":"2024-02-28T19:49:32.159609Z","iopub.status.idle":"2024-02-28T19:49:32.168794Z","shell.execute_reply":"2024-02-28T19:49:32.167612Z","shell.execute_reply.started":"2024-02-28T19:49:32.159975Z"},"id":"hdp0yz7mJsy_","trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","from datasets import Dataset, DatasetDict\n","from transformers import BartTokenizer, BartForConditionalGeneration, T5ForConditionalGeneration, AutoTokenizer\n","from torch.utils.data import DataLoader\n","from nltk.metrics.distance import edit_distance\n","import pytorch_lightning as pl\n","from torch.optim import AdamW\n","from dataclasses import dataclass\n","\n","# Set up wandb environment\n","os.environ[\"WANDB_PROJECT\"] = \"Seq2SeqZip\"\n","\n","# Seed for reproducibility\n","SEED = 999\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T19:49:32.170524Z","iopub.status.busy":"2024-02-28T19:49:32.170223Z","iopub.status.idle":"2024-02-28T19:49:32.469851Z","shell.execute_reply":"2024-02-28T19:49:32.469062Z","shell.execute_reply.started":"2024-02-28T19:49:32.170498Z"},"id":"1nFsQ146JszA","trusted":true},"outputs":[],"source":["# Function to load and preprocess dataset\n","def load_and_preprocess_data(filepath):\n","    df = pd.read_csv(filepath)[:15000]\n","    df[['deflate_hex', 'text_hex', 'text']] += \"</s>\"\n","    ds = Dataset.from_pandas(df)\n","    ds_train_test = ds.train_test_split(test_size=0.2, seed=SEED)\n","    ds_test_dev = ds_train_test['test'].train_test_split(test_size=0.5, seed=SEED)\n","    return DatasetDict({\n","        'train': ds_train_test['train'],\n","        'valid': ds_test_dev['train'],\n","        'test': ds_test_dev['test']\n","    })\n","\n","ds_splits = load_and_preprocess_data('/kaggle/input/full-dataset/randomized_shorthex2hex.csv')"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T19:49:32.472681Z","iopub.status.busy":"2024-02-28T19:49:32.472307Z","iopub.status.idle":"2024-02-28T19:49:32.480689Z","shell.execute_reply":"2024-02-28T19:49:32.479687Z","shell.execute_reply.started":"2024-02-28T19:49:32.472646Z"},"id":"u-PSQfryJszA","trusted":true},"outputs":[],"source":["@dataclass\n","class DataCollatorSeq2SeqWithPadding:\n","    tokenizer: AutoTokenizer\n","\n","    def __call__(self, dataset_elements):\n","        inputs = [x[\"text_hex\"] for x in dataset_elements]\n","        outputs = [x[\"deflate_hex\"] for x in dataset_elements]\n","        input_features = self.tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length = MAX_SEQ_LEN)\n","        output_features = self.tokenizer(outputs, return_tensors=\"pt\", padding=True, truncation=True, max_length = MAX_SEQ_LEN)[\"input_ids\"]\n","        output_features[output_features == self.tokenizer.pad_token_id] = -100\n","        return {\"input_ids\": input_features[\"input_ids\"], \"attention_mask\": input_features[\"attention_mask\"], \"labels\": output_features}\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T19:49:32.482512Z","iopub.status.busy":"2024-02-28T19:49:32.482073Z","iopub.status.idle":"2024-02-28T19:49:38.620632Z","shell.execute_reply":"2024-02-28T19:49:38.619796Z","shell.execute_reply.started":"2024-02-28T19:49:32.482474Z"},"id":"PGd5_RNtJszA","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e9cf375bdf848ed83877ab771575b3f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e38dd2d915b4004896ab9ac95df1974","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2aedc94f85f742c68e03f5db5ed0343a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fb36b544d9c44d18815aae6c6109cd5","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39a4074331f94b1d99c565508e47a8ea","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#MODEL CHOICES: bart-base, bart-large, t5-base\n","MODEL = \"t5-base\"\n","\n","if (MODEL == \"bart-base\"):\n","    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n","    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n","\n","elif (MODEL == \"bart-large\"):\n","    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n","    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n","\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n","    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n","\n","data_collator = DataCollatorSeq2SeqWithPadding(tokenizer = tokenizer)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T19:49:38.622061Z","iopub.status.busy":"2024-02-28T19:49:38.621748Z","iopub.status.idle":"2024-02-28T19:49:38.629534Z","shell.execute_reply":"2024-02-28T19:49:38.628547Z","shell.execute_reply.started":"2024-02-28T19:49:38.622028Z"},"id":"QuX0gGj2JszA","trusted":true},"outputs":[],"source":["def compute_metrics(preds, labels, tokenizer):\n","    # Ensure labels with -100 are replaced by pad_token_id\n","    labels = torch.where(labels == -100, tokenizer.pad_token_id, labels)\n","\n","    # Convert tensors to lists for decoding if they're not already in CPU\n","    if torch.is_tensor(preds):\n","        preds = preds.detach().cpu().tolist()\n","    if torch.is_tensor(labels):\n","        labels = labels.detach().cpu().tolist()\n","\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    distances = [edit_distance(p, l) for p, l in zip(decoded_preds, decoded_labels)]\n","    avg_distance = np.mean(distances)\n","    count_unzippable = distances.count(0)\n","\n","    return {\"average_edit_distance\": avg_distance, \"count_unzippable\": count_unzippable}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T19:49:38.631538Z","iopub.status.busy":"2024-02-28T19:49:38.631136Z"},"id":"mvEo6XxsJszA","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d31833b6581e46b394764400881a91fc","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7c841f5d6bf430eac0ba4e703e6b034","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["BATCH_SIZE = 16\n","MAX_EPOCHS = 5\n","LEARNING_RATE = 1e-4\n","WEIGHT_DECAY = 1e-2\n","MAX_SEQ_LEN = 256\n","\n","class Seq2Seq(pl.LightningModule):\n","    def __init__(self, tokenizer, model, data_collator):\n","        super().__init__()\n","        self.tokenizer = tokenizer\n","        self.model = model\n","        self.data_collator = data_collator\n","\n","    def forward(self, input_ids, attention_mask=None, labels=None):\n","        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","\n","    def training_step(self, batch, batch_idx):\n","        outputs = self.forward(**batch)\n","        self.log('train_loss', outputs.loss, prog_bar=True, logger=True)\n","        return outputs.loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        outputs = self.forward(**batch)\n","        self.log('val_loss', outputs.loss, prog_bar=True, logger=True)\n","\n","        preds = torch.argmax(outputs.logits, dim=-1)\n","        metrics = compute_metrics(preds, batch['labels'], self.tokenizer)\n","        for key, value in metrics.items():\n","            self.log(f'{key}', value, prog_bar=True, logger=True)\n","\n","        return outputs.loss\n","\n","    def test_step(self, batch, batch_idx):\n","        outputs = self.forward(**batch)\n","        self.log('test_loss', outputs.loss, prog_bar=True, logger=True)\n","        \n","        preds = torch.argmax(outputs.logits, dim=-1)\n","        metrics = compute_metrics(preds, batch['labels'], self.tokenizer)\n","        \n","        for key, value in metrics.items():\n","            self.log(f'{key}', value, prog_bar=True, logger=True)\n","        return outputs.loss\n","\n","    def configure_optimizers(self):\n","        # Directly use learning rate and weight decay values here\n","        return AdamW(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","\n","trainer = pl.Trainer(\n","    precision='16-mixed',\n","    max_epochs=MAX_EPOCHS,\n","    enable_progress_bar=True,\n",")\n","\n","train_dataloader = DataLoader(ds_splits[\"train\"], batch_size=BATCH_SIZE, shuffle = True, collate_fn=data_collator, num_workers = 3)\n","valid_dataloader = DataLoader(ds_splits[\"valid\"], batch_size=BATCH_SIZE, shuffle = False, collate_fn=data_collator, num_workers = 3)\n","test_dataloader = DataLoader(ds_splits[\"test\"], batch_size=BATCH_SIZE, shuffle = False, collate_fn=data_collator, num_workers = 3)\n","\n","seq2seq_model = Seq2Seq(tokenizer, model, data_collator)\n","trainer.fit(seq2seq_model, train_dataloader, valid_dataloader)\n","\n","trainer.test(seq2seq_model, test_dataloader)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4447546,"sourceId":7632616,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
