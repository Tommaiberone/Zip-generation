{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7711642,
          "sourceType": "datasetVersion",
          "datasetId": 4503096
        },
        {
          "sourceId": 7834971,
          "sourceType": "datasetVersion",
          "datasetId": 4592427
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3153f70318d461e97e4f667adcf7c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eac34a5c9ef47b79b9fb6ea21bb36c9",
              "IPY_MODEL_53ee2fd7914242a38453bb5c6c3c838c",
              "IPY_MODEL_9033b989211046af8e0058649cd2a0d3"
            ],
            "layout": "IPY_MODEL_6743ce227d0d4f2bbc1c5855f3b012fc"
          }
        },
        "6eac34a5c9ef47b79b9fb6ea21bb36c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a511cd9d32fa410da98ccbea63b401fa",
            "placeholder": "​",
            "style": "IPY_MODEL_3b98af18eabb4731b476269fef4a8d28",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "53ee2fd7914242a38453bb5c6c3c838c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1421e2b303614245adf21d07b39d4261",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ca1fde836904c23ae6fce60c9f5acb4",
            "value": 2
          }
        },
        "9033b989211046af8e0058649cd2a0d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42d41d7545a345808a26ff2879e48859",
            "placeholder": "​",
            "style": "IPY_MODEL_d5afc7446ba64d26aee8162707ebd9f7",
            "value": " 2/2 [00:00&lt;00:00,  2.37it/s]"
          }
        },
        "6743ce227d0d4f2bbc1c5855f3b012fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "a511cd9d32fa410da98ccbea63b401fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b98af18eabb4731b476269fef4a8d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1421e2b303614245adf21d07b39d4261": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ca1fde836904c23ae6fce60c9f5acb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42d41d7545a345808a26ff2879e48859": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5afc7446ba64d26aee8162707ebd9f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba6e9b8e9c494d6d982a23abc9bb37e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4cc0bd9f56994ea1af6aa217f68cffb7",
              "IPY_MODEL_06718797fb694bc3821446914a6cec6d",
              "IPY_MODEL_f1a9cda092ab4ccfa05b373904047932"
            ],
            "layout": "IPY_MODEL_ed04d22b43024309b75aa046e16bab84"
          }
        },
        "4cc0bd9f56994ea1af6aa217f68cffb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88eebae2fc1148f4a2076b6e6a56d531",
            "placeholder": "​",
            "style": "IPY_MODEL_1ee05b56eb8f4d5583619f68ee0c4b4c",
            "value": "Epoch 0:  47%"
          }
        },
        "06718797fb694bc3821446914a6cec6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd954e43c0cf4f4db4d6a2aa1f64c05c",
            "max": 256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f887261880746e28595248b343cb6b7",
            "value": 120
          }
        },
        "f1a9cda092ab4ccfa05b373904047932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e4e5faa33124825a5fda66e9d1e5846",
            "placeholder": "​",
            "style": "IPY_MODEL_81b83b955635466b9f7de8727b2ead1e",
            "value": " 120/256 [00:20&lt;00:22,  5.99it/s, v_num=0, train_loss=1.770]"
          }
        },
        "ed04d22b43024309b75aa046e16bab84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "88eebae2fc1148f4a2076b6e6a56d531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ee05b56eb8f4d5583619f68ee0c4b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd954e43c0cf4f4db4d6a2aa1f64c05c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f887261880746e28595248b343cb6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e4e5faa33124825a5fda66e9d1e5846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81b83b955635466b9f7de8727b2ead1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and installation\n"
      ],
      "metadata": {
        "id": "JkKtT4GK-7y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets transformers accelerate evaluate wandb nltk pandas lightning"
      ],
      "metadata": {
        "id": "woNI1XG6H-Bb",
        "execution": {
          "iopub.status.busy": "2024-03-14T14:12:47.102006Z",
          "iopub.execute_input": "2024-03-14T14:12:47.102895Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, T5ForConditionalGeneration, AutoTokenizer\n",
        "import lightning as L\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "SEED = 124\n",
        "\n",
        "#DATA USED\n",
        "SHORT = False\n",
        "RANDOMIZED_SHORT = True\n",
        "MEDIUM = False\n",
        "\n",
        "MAX_SEQ_LEN = 512 if MEDIUM else 256\n",
        "\n",
        "#MODEL USED\n",
        "FEEDFORWARD = False\n",
        "FEEDFORWARD_WITH_ATTENTION = False\n",
        "CONV1D = False\n",
        "RNN = True\n",
        "SEQ2SEQ = False\n",
        "\n",
        "#MODEL CHOICES FOR SEQ2SEQ: bart-base, bart-large, t5-base\n",
        "MODEL = \"bart-base\"\n",
        "\n",
        "#RNN MODELS AND HYPERPARAMETERS\n",
        "BIDIRECTIONAL = False\n",
        "RNN_TYPE = 'RNN'  # Options: 'LSTM', 'GRU', 'RNN'\n",
        "\n",
        "#HYPERPARAMETERS\n",
        "EMBED_DIM = 128\n",
        "HIDDEN_DIM = 512\n",
        "LEARNING_RATE = 5e-4\n",
        "DROPOUT_RATE = 0.5\n",
        "NUM_HEADS = 4\n",
        "NUM_LAYERS = 4\n",
        "WEIGHT_DECAY = 0.01\n",
        "MAX_EPOCHS = 5\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "L.seed_everything(SEED)"
      ],
      "metadata": {
        "id": "tBg7g-c1A7kR",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9311301-0c05-4944-8e1b-f78ede35ddce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 124\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 124\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_values = [FEEDFORWARD, FEEDFORWARD_WITH_ATTENTION, CONV1D, RNN, SEQ2SEQ]\n",
        "num_true = sum(models_values)\n",
        "\n",
        "# Check if only one value is True\n",
        "if num_true == 1:\n",
        "    print(\"OK\")\n",
        "else:\n",
        "    print(\"ATTENTION! SELECT ONLY ONE MODEL TO RUN\")\n",
        "    ## Using this so that is more than one model is selected the execution does not continue\n",
        "    print(UNDEFINED_VARIABLE_TO_LET_THE_NOTEBOOK_CRASH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9xAcbOfEyxw",
        "outputId": "a0306f86-9cf6-4449-f7fc-16829764a3b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "afxIsaS-8AhR",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b33be5-e839-4512-bca1-d03e071827e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "mwbZbYZN_05Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Mapping from token to id used for encoding hexadecimal strings\n",
        "token2id = {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6, \"7\": 7, \"8\": 8, \"9\": 9, \"a\": 10, \"b\": 11, \"c\": 12, \"d\": 13, \"e\": 14, \"f\": 15, \"P\":16, \"S\": 17, \"E\":18 }\n",
        "def create_id2token_vocab(token_to_id):\n",
        "    id2token = {}\n",
        "    for token, id in token_to_id.items():\n",
        "        id2token[id] = token\n",
        "\n",
        "    return id2token\n",
        "\n",
        "id2token = create_id2token_vocab(token2id)\n",
        "\n",
        "## INIZIALIZE OUTPUT DIM NOW THAT I KNOW THE LENGTH OF THE TOKEN2ID DICTIONARY\n",
        "OUTPUT_DIM = len(token2id)"
      ],
      "metadata": {
        "id": "y5zOqIHW-mTV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/datasets.zip https://github.com/Tommaiberone/Zip-generation/raw/main/Datasets/datasets.zip\n",
        "!unzip -o /content/datasets.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41-sA-aM-G7v",
        "outputId": "96068bf3-3041-435f-c7c0-749e1d68780a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-29 17:00:01--  https://github.com/Tommaiberone/Zip-generation/raw/main/Datasets/datasets.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Tommaiberone/Zip-generation/main/Datasets/datasets.zip [following]\n",
            "--2024-03-29 17:00:01--  https://raw.githubusercontent.com/Tommaiberone/Zip-generation/main/Datasets/datasets.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8767992 (8.4M) [application/zip]\n",
            "Saving to: ‘/content/datasets.zip’\n",
            "\n",
            "/content/datasets.z 100%[===================>]   8.36M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-03-29 17:00:01 (99.1 MB/s) - ‘/content/datasets.zip’ saved [8767992/8767992]\n",
            "\n",
            "Archive:  /content/datasets.zip\n",
            "  inflating: mediumhex2hex.csv       \n",
            "  inflating: randomized_shorthex2hex.csv  \n",
            "  inflating: shorthex2hex.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SHORT:\n",
        "  df = pd.read_csv('/content/mediumhex2hex.csv')\n",
        "elif RANDOMIZED_SHORT:\n",
        "  df = pd.read_csv('/content/randomized_shorthex2hex.csv')\n",
        "elif MEDIUM:\n",
        "  df = pd.read_csv('/content/shorthex2hex.csv')\n",
        "\n",
        "\n",
        "df = df[:40960]"
      ],
      "metadata": {
        "id": "f-sQt-E_Arl2",
        "trusted": true
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RNN:\n",
        "  df['text_hex'] = 'S' + df['text_hex'] + 'E'\n",
        "  df['deflate_hex'] = 'S' + df['deflate_hex'] + 'E'\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "aT-D7npnp_3p",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "cb8ceada-96f1-47a1-d2b7-2b5e5453c265"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      text                                          text_hex  \\\n",
              "0            this is not a                      S74686973206973206e6f742061E   \n",
              "1  and gives a comforting,  S616e64206769766573206120636f6d666f7274696e672cE   \n",
              "2  killer). While some may  S6b696c6c6572292e205768696c6520736f6d65206d6179E   \n",
              "3          in his closet &                  S696e2068697320636c6f7365742026E   \n",
              "4       film to watch. Mr.            S66696c6d20746f2077617463682e204d722eE   \n",
              "\n",
              "                                         deflate_hex  \n",
              "0           S789c2bc9c82c5600a2bcfc1285440021fe04a7E  \n",
              "1  S789c4bcc4b5148cf2c4b2d56485448cecf4dcb2f2ac9c...  \n",
              "2  S789ccbceccc9492dd2d45308cfc8cc495528cecf4d55c...  \n",
              "3   S789ccbcc53c8c82c5648cec92f4e2d515003002b16052cE  \n",
              "4  S789c4bcbccc95528c957284f2c49ced053f02dd203003...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a2cb191-ce8e-424d-8f3f-4168a9a5895b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_hex</th>\n",
              "      <th>deflate_hex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>this is not a</td>\n",
              "      <td>S74686973206973206e6f742061E</td>\n",
              "      <td>S789c2bc9c82c5600a2bcfc1285440021fe04a7E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>and gives a comforting,</td>\n",
              "      <td>S616e64206769766573206120636f6d666f7274696e672cE</td>\n",
              "      <td>S789c4bcc4b5148cf2c4b2d56485448cecf4dcb2f2ac9c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>killer). While some may</td>\n",
              "      <td>S6b696c6c6572292e205768696c6520736f6d65206d6179E</td>\n",
              "      <td>S789ccbceccc9492dd2d45308cfc8cc495528cecf4d55c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>in his closet &amp;</td>\n",
              "      <td>S696e2068697320636c6f7365742026E</td>\n",
              "      <td>S789ccbcc53c8c82c5648cec92f4e2d515003002b16052cE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>film to watch. Mr.</td>\n",
              "      <td>S66696c6d20746f2077617463682e204d722eE</td>\n",
              "      <td>S789c4bcbccc95528c957284f2c49ced053f02dd203003...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a2cb191-ce8e-424d-8f3f-4168a9a5895b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a2cb191-ce8e-424d-8f3f-4168a9a5895b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a2cb191-ce8e-424d-8f3f-4168a9a5895b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7ec1ac38-ccda-4fba-b22e-274111f95d46\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ec1ac38-ccda-4fba-b22e-274111f95d46')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7ec1ac38-ccda-4fba-b22e-274111f95d46 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 40960,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40690,\n        \"samples\": [\n          \"be relegated to subserviant\",\n          \"of Power Rangers contains\",\n          \"her in whatever situation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_hex\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40690,\n        \"samples\": [\n          \"S62652072656c65676174656420746f207375627365727669616e74E\",\n          \"S6f6620506f7765722052616e6765727320636f6e7461696e73E\",\n          \"S68657220696e20776861746576657220736974756174696f6eE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"deflate_hex\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40690,\n        \"samples\": [\n          \"S789c4b4a55284acd494d4f2c494d5128c957282e4d2a4e2d2acb4ccc2b01008d3f0a6eE\",\n          \"S789ccb4f5308c82f4f2d52084acc4b4f2d2a5648cecf2b49cccc2b0600769f0974E\",\n          \"S789ccb482d52c8cc5328cf482c492d03b28b334b4a134b32f3f3007a4d09bdE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using the standard \\<EOS> and \\<SOS> tags we're using the letter S and E since they are not present in the vocabulary"
      ],
      "metadata": {
        "id": "bVUV1WDuvi7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if SEQ2SEQ:\n",
        "    df = df[:15000]\n",
        "    df[['deflate_hex', 'text_hex', 'text']] += \"</s>\"\n",
        "\n",
        "ds = Dataset.from_pandas(df)\n",
        "ds_train_test = ds.train_test_split(test_size=0.2, seed=SEED)\n",
        "ds_test_dev = ds_train_test['test'].train_test_split(test_size=0.5, seed=SEED)\n",
        "ds_splits = DatasetDict({\n",
        "    'train': ds_train_test['train'],\n",
        "    'valid': ds_test_dev['train'],\n",
        "    'test': ds_test_dev['test']\n",
        "})"
      ],
      "metadata": {
        "id": "LE9OmLvsIdlf",
        "trusted": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data tokenization"
      ],
      "metadata": {
        "id": "YBdD8nIBrL30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if SEQ2SEQ:\n",
        "    if (MODEL == \"bart-base\"):\n",
        "        tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "    elif (MODEL == \"bart-large\"):\n",
        "        tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
        "\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "ul8yGNNv9zKn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "\n",
        "    if FEEDFORWARD or FEEDFORWARD_WITH_ATTENTION or CONV1D:\n",
        "\n",
        "        texts = [elem['text_hex'] for elem in batch]\n",
        "        encoded_hexs = [[token2id[char] for char in text] for text in texts]\n",
        "\n",
        "        outputs = [elem['deflate_hex'] for elem in batch]\n",
        "        encoded_outputs = [[token2id[char] for char in output] for output in outputs]\n",
        "\n",
        "        ## Pad the sequences to MAX_SEQ_LEN chars with the padding token\n",
        "        padded_hex = [torch.Tensor(encoded_hex + [token2id[\"P\"]] * (MAX_SEQ_LEN - len(encoded_hex))) for encoded_hex in encoded_hexs]\n",
        "        padded_outputs = [torch.Tensor(encoded_output + [token2id[\"P\"]] * (MAX_SEQ_LEN - len(encoded_output))) for encoded_output in encoded_outputs]\n",
        "\n",
        "        ## Stack the sequences\n",
        "        padded_hex = torch.stack(padded_hex).long()\n",
        "        padded_outputs = torch.stack(padded_outputs).long()\n",
        "\n",
        "\n",
        "        return {\n",
        "            'inputs': padded_hex,\n",
        "            'outputs': padded_outputs\n",
        "        }\n",
        "\n",
        "    elif RNN:\n",
        "\n",
        "        ## Dynamic padding for RNNs\n",
        "        def pad_sequences(sequences, maxlen, value=token2id['P']):\n",
        "            padded_sequences = []\n",
        "            for sequence in sequences:\n",
        "                padded_sequence = sequence[:maxlen]\n",
        "                padded_sequence.extend([value] * (maxlen - len(padded_sequence)))\n",
        "\n",
        "                padded_sequence = sequence +  [value] * (maxlen - len(sequence))\n",
        "                padded_sequences.append(padded_sequence)\n",
        "\n",
        "            return padded_sequences\n",
        "\n",
        "\n",
        "        texts = [elem['text_hex'] for elem in batch]\n",
        "        encoded_hex = [[token2id[x] for x in hex] for hex in texts]\n",
        "\n",
        "\n",
        "        outputs = [elem['deflate_hex'] for elem in batch]\n",
        "        encoded_outputs = [[token2id[x] for x in hex] for hex in outputs]\n",
        "\n",
        "\n",
        "        maxlen = 0\n",
        "        for seq in encoded_hex:\n",
        "            if len(seq) > maxlen:\n",
        "                maxlen = len(seq)\n",
        "        for seq in encoded_outputs:\n",
        "            if len(seq) > maxlen:\n",
        "                maxlen = len(seq)\n",
        "\n",
        "        padded_encoded_hex = pad_sequences(encoded_hex, maxlen)\n",
        "        padded_encoded_outputs = pad_sequences(encoded_outputs, maxlen)\n",
        "\n",
        "\n",
        "        return {\n",
        "            'inputs': torch.tensor(padded_encoded_hex),\n",
        "            \"outputs\": torch.tensor(padded_encoded_outputs)\n",
        "        }\n",
        "\n",
        "    elif SEQ2SEQ:\n",
        "        inputs = [x[\"text_hex\"] for x in batch]\n",
        "        outputs = [x[\"deflate_hex\"] for x in batch]\n",
        "        input_features = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SEQ_LEN)\n",
        "        output_features = tokenizer(outputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SEQ_LEN)[\"input_ids\"]\n",
        "        output_features[output_features == tokenizer.pad_token_id] = -100\n",
        "        return {\"input_ids\": input_features[\"input_ids\"], \"attention_mask\": input_features[\"attention_mask\"], \"labels\": output_features}\n"
      ],
      "metadata": {
        "id": "VF82QijJt4nw",
        "trusted": true
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing dataloaders"
      ],
      "metadata": {
        "id": "YpUMB8N1_l5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(ds_splits['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers = 3)\n",
        "val_dataloader = DataLoader(ds_splits['valid'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers = 3)\n",
        "test_dataloader = DataLoader(ds_splits['test'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers = 3)\n",
        "\n",
        "torch.set_printoptions(profile=\"full\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "imBbEVs39zKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5afed756-6abc-4bce-d7f2-b895ed56a091"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation functions\n",
        "\n",
        "We're using the nltk library to compute the edit distance (Levenshtein distance) between the predicted string and the target/gold string."
      ],
      "metadata": {
        "id": "TGAexQaB_rIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "def decode_output(output):\n",
        "    return ''.join([id2token[int(id)] for id in output])\n",
        "\n",
        "def decode_input(input):\n",
        "    return ''.join([id2token[int(id)] for id in input])\n",
        "\n",
        "## function used to compute metrics for Seq2Seq models (bart/t5)\n",
        "def compute_seq2seq_metrics(preds, labels, tokenizer):\n",
        "    # Ensure labels with -100 are replaced by pad_token_id\n",
        "    labels = torch.where(labels == -100, tokenizer.pad_token_id, labels)\n",
        "\n",
        "    # Convert tensors to lists and detach them from cuda\n",
        "    if torch.is_tensor(preds):\n",
        "        preds = preds.detach().cpu().tolist()\n",
        "    if torch.is_tensor(labels):\n",
        "        labels = labels.detach().cpu().tolist()\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    distances = [edit_distance(p, l) for p, l in zip(decoded_preds, decoded_labels)]\n",
        "    avg_distance = np.mean(distances)\n",
        "    count_unzippable = distances.count(0)\n",
        "\n",
        "    return {\"average_edit_distance\": avg_distance, \"count_unzippable\": count_unzippable}\n",
        "\n",
        "## function used to compute metrics for all the other models\n",
        "def evaluate(_device, _print, _training):\n",
        "\n",
        "    model.eval()\n",
        "    total_distance = 0\n",
        "    total = 0\n",
        "\n",
        "    distances_list = []\n",
        "\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "\n",
        "        if FEEDFORWARD or FEEDFORWARD_WITH_ATTENTION or CONV1D:\n",
        "            x = batch[\"inputs\"].to(_device)\n",
        "            y = batch[\"outputs\"].to(_device)\n",
        "\n",
        "            y_hat = model(x, y)\n",
        "            y_hat = torch.argmax(y_hat, dim=-1)\n",
        "\n",
        "            output = decode_output(y[0])\n",
        "            output_hat = decode_output(y_hat[0])\n",
        "\n",
        "            output = [x for x in output if x != \"P\"]\n",
        "            output_hat = [x for x in output_hat if x != \"P\"]\n",
        "\n",
        "            distance = edit_distance(output, output_hat)\n",
        "\n",
        "        elif RNN:\n",
        "            x = batch[\"inputs\"].transpose(0,1).to(_device)\n",
        "            y = batch[\"outputs\"].transpose(0,1).to(_device)\n",
        "\n",
        "            y_hat = model(x, y)\n",
        "            y_hat = torch.argmax(y_hat, dim=-1)\n",
        "\n",
        "            y = y.transpose(0,1)\n",
        "            y_hat = y_hat.transpose(0,1)\n",
        "\n",
        "            assert len(y) == len(y_hat)\n",
        "\n",
        "            for i in range(len(y)):\n",
        "                output = decode_output(y[i])\n",
        "                output_hat = decode_output(y_hat[i])\n",
        "\n",
        "                ## Remove padding\n",
        "                output = [x for x in output if x != \"P\"]\n",
        "                output_hat = [x for x in output_hat if x != \"P\"]\n",
        "\n",
        "                ## Save the index of the first EOS token, if any. Else consider all the string\n",
        "                first_eos_index = len(output_hat)\n",
        "                for i in range(len(output_hat)):\n",
        "                    if output_hat[i] == \"E\":\n",
        "                        first_eos_index = i\n",
        "                        break\n",
        "\n",
        "                # Remove SOS token\n",
        "                output = output[1:]\n",
        "                output_hat = output_hat[1:first_eos_index]\n",
        "\n",
        "                ## Compute distance\n",
        "                distance = edit_distance(output, output_hat)\n",
        "                distances_list.append(distance)\n",
        "\n",
        "        if _print:\n",
        "            print(f\"output = {output}\")\n",
        "            print(f\"output_hat = {output_hat}\")\n",
        "\n",
        "        total_distance += distance\n",
        "        total += 1\n",
        "\n",
        "        if distance == 0:\n",
        "            print(f\"DISTANCE = 0!\")\n",
        "            print(f\"output = {output}\")\n",
        "            print(f\"output_hat = {output_hat}\")\n",
        "\n",
        "        if _training:\n",
        "            return total_distance/total\n",
        "\n",
        "    return (total_distance/total, distances_list)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9Dj2fBhI9zKo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "6IDexjaaCS1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "## In this class there are the following models:\n",
        "## 1. Vanilla FeedForward\n",
        "## 2. FeedForward with Attention\n",
        "## 3. Conv1D\n",
        "## 4. Recurrent models\n",
        "## You can switch between those models using the parameters present in the first cell of this Notebook\n",
        "\n",
        "class FeedForward(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, input_dim=MAX_SEQ_LEN, embed_dim = EMBED_DIM, hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM, learning_rate=LEARNING_RATE,\n",
        "                 dropout_rate=DROPOUT_RATE, bidirectional=BIDIRECTIONAL, num_layers=NUM_LAYERS, optimizer_type=AdamW, scheduler_type=StepLR,\n",
        "                 scheduler_step_size=5, scheduler_gamma=0.1):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        if FEEDFORWARD or FEEDFORWARD_WITH_ATTENTION:\n",
        "            self.embed = nn.Embedding(input_dim, embed_dim)\n",
        "            self.positional_embeddings = nn.Parameter(torch.zeros(BATCH_SIZE, input_dim, embed_dim))\n",
        "            nn.init.normal_(self.positional_embeddings, mean=0, std=embed_dim ** -0.5)  # Initialize positional embeddings\n",
        "            self.self_attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=NUM_HEADS, dropout=dropout_rate, batch_first = True)\n",
        "            self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "            self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "            self.fc2 = nn.Linear(hidden_dim, hidden_dim//2)\n",
        "            self.norm2 = nn.LayerNorm(hidden_dim//2)\n",
        "            self.fc3 = nn.Linear(hidden_dim//2, output_dim)\n",
        "\n",
        "        elif CONV1D:\n",
        "\n",
        "            # Embedding layer to transform dictionary indices into dense vectors\n",
        "            self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=embed_dim)\n",
        "\n",
        "            # Convolutional layers\n",
        "            self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=3, padding=1)\n",
        "\n",
        "            # Fully connected layers for classification\n",
        "            self.fc1 = nn.Linear(embed_dim, embed_dim)\n",
        "            self.fc2 = nn.Linear(embed_dim, output_dim)\n",
        "\n",
        "            # Hyperparameters\n",
        "            self.learning_rate = learning_rate\n",
        "\n",
        "        elif RNN:\n",
        "            self.rnn_type = RNN_TYPE\n",
        "            self.embedding = nn.Embedding(output_dim, embed_dim, padding_idx=token2id['P'])\n",
        "\n",
        "            if self.rnn_type == 'LSTM':\n",
        "                rnn_cell = nn.LSTM\n",
        "            elif self.rnn_type == 'GRU':\n",
        "                rnn_cell = nn.GRU\n",
        "            else:  # Default to RNN if neither LSTM nor GRU is selected\n",
        "                rnn_cell = nn.RNN\n",
        "\n",
        "            self.encoder_rnn = rnn_cell(embed_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_rate if num_layers > 1 else 0)\n",
        "            self.decoder_rnn = rnn_cell(embed_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_rate if num_layers > 1 else 0)\n",
        "\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "            self.output_dim = output_dim\n",
        "            self.linear = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "            self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x, target, teacher_forcing_ratio=0.5):\n",
        "\n",
        "        if FEEDFORWARD or FEEDFORWARD_WITH_ATTENTION:\n",
        "\n",
        "            # Embedding\n",
        "            x = self.embed(x)  # Shape: [Batch, Seq_len, Embed_dim]\n",
        "\n",
        "            if (FEEDFORWARD_WITH_ATTENTION):\n",
        "\n",
        "                # Add positional embeddings\n",
        "                positions = self.positional_embeddings\n",
        "                x = x + positions\n",
        "\n",
        "                # Self-attention\n",
        "                attn_output, _ = self.self_attention(x, x, x)\n",
        "\n",
        "                x = torch.relu(self.norm1(self.fc1(attn_output)))\n",
        "\n",
        "            else:\n",
        "                x = torch.relu(self.norm1(self.fc1(x)))\n",
        "\n",
        "            x = self.dropout(x)\n",
        "            x = torch.relu(self.norm2(self.fc2(x)))\n",
        "            x = self.fc3(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "        elif CONV1D:\n",
        "\n",
        "            # Embedding layer\n",
        "            x = self.embedding(x)\n",
        "\n",
        "            # Transpose from (batch_size, sequence_length, embedding_dim) to (batch_size, embedding_dim, sequence_length)\n",
        "            x = x.permute(0, 2, 1)\n",
        "\n",
        "            x = torch.relu(self.conv1(x))\n",
        "\n",
        "            x = x.permute(0, 2, 1)\n",
        "\n",
        "            x = torch.relu(self.fc1(x))\n",
        "\n",
        "            x = self.fc2(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "        elif RNN:\n",
        "            target_len = target.shape[0]\n",
        "            batch_size = target.shape[1]\n",
        "            target_vocab_size = self.output_dim\n",
        "\n",
        "            outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
        "\n",
        "            x = self.dropout(self.embedding(x))\n",
        "            rnn_output, h = self.encoder_rnn(x)\n",
        "\n",
        "            x = target[0]\n",
        "            for t in range(1, target_len):\n",
        "                x = self.dropout(self.embedding(x.unsqueeze(0)))\n",
        "                out, h = self.decoder_rnn(x, h if self.rnn_type in ['LSTM', 'GRU'] else None)\n",
        "                predictions = self.linear(out)\n",
        "                predictions = predictions.squeeze(0)\n",
        "                outputs[t] = predictions\n",
        "                pred = predictions.argmax(1)\n",
        "                x = target[t] if random.random() < teacher_forcing_ratio else pred\n",
        "\n",
        "            return outputs\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = self.hparams.optimizer_type(self.parameters(), lr=self.hparams.learning_rate)\n",
        "        scheduler = self.hparams.scheduler_type(optimizer, step_size=self.hparams.scheduler_step_size, gamma=self.hparams.scheduler_gamma)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def step(self, batch):\n",
        "        if (FEEDFORWARD or FEEDFORWARD_WITH_ATTENTION or CONV1D):\n",
        "            x = batch[\"inputs\"]\n",
        "            y = batch[\"outputs\"]\n",
        "            y = y.view(y.shape[0] * y.shape[1])\n",
        "            y_hat = self(x, y)\n",
        "            y_hat = y_hat.view(y_hat.shape[0] * y_hat.shape[1], y_hat.shape[2])\n",
        "\n",
        "        elif RNN:\n",
        "            inputs, targets = batch['inputs'], batch['outputs']\n",
        "            inputs = inputs.transpose(0, 1)\n",
        "            targets = targets.transpose(0, 1)\n",
        "\n",
        "            output = self(inputs, targets)\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            y_hat = output.reshape(-1, output_dim)\n",
        "            y = targets.reshape(-1)\n",
        "\n",
        "        loss = self.loss(y_hat, y)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.step(batch)\n",
        "        self.log('train_loss', loss, prog_bar = True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.step(batch)\n",
        "        self.log('val_loss', loss, prog_bar = True)\n",
        "        return loss\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "8a2nJxCC9zKo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Models"
      ],
      "metadata": {
        "id": "Lm98JZPQDOhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Recurrent(pl.LightningModule):\n",
        "      def __init__(self, input_dim=MAX_SEQ_LEN, embed_dim = EMBED_DIM, hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM, learning_rate=LEARNING_RATE,\n",
        "                 dropout_rate=DROPOUT_RATE, bidirectional=BIDIRECTIONAL, num_layers=NUM_LAYERS, optimizer_type=AdamW, scheduler_type=StepLR,\n",
        "                 scheduler_step_size=5, scheduler_gamma=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.rnn_type = RNN_TYPE\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim, padding_idx=token2id['P'])\n",
        "\n",
        "        if self.rnn_type == 'LSTM':\n",
        "          rnn_cell = nn.LSTM\n",
        "        elif self.rnn_type == 'GRU':\n",
        "          rnn_cell = nn.GRU\n",
        "        else:  # Default to RNN if neither LSTM nor GRU is selected\n",
        "          rnn_cell = nn.RNN\n",
        "\n",
        "        self.encoder_rnn = rnn_cell(embed_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_rate if num_layers > 1 else 0)\n",
        "        self.decoder_rnn = rnn_cell(embed_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_rate if num_layers > 1 else 0)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.output_dim = output_dim\n",
        "        self.linear = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "      def forward(self, x, target, teacher_forcing_ratio=0.5):\n",
        "        target_len = target.shape[0]\n",
        "        batch_size = target.shape[1]\n",
        "        target_vocab_size = self.output_dim\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
        "\n",
        "        x = self.dropout(self.embedding(x))\n",
        "        rnn_output, h = self.encoder_rnn(x)\n",
        "\n",
        "        x = target[0]\n",
        "        for t in range(1, target_len):\n",
        "          x = self.dropout(self.embedding(x.unsqueeze(0)))\n",
        "          out, h = self.decoder_rnn(x, h if self.rnn_type in ['LSTM', 'GRU'] else None)\n",
        "          predictions = self.linear(out)\n",
        "          predictions = predictions.squeeze(0)\n",
        "          outputs[t] = predictions\n",
        "          pred = predictions.argmax(1)\n",
        "          x = target[t] if random.random() < teacher_forcing_ratio else pred\n",
        "\n",
        "        return outputs\n",
        "\n",
        "      def configure_optimizers(self):\n",
        "        optimizer = self.hparams.optimizer_type(self.parameters(), lr=self.hparams.learning_rate)\n",
        "        scheduler = self.hparams.scheduler_type(optimizer, step_size=self.hparams.scheduler_step_size, gamma=self.hparams.scheduler_gamma)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "      def step(self, batch):\n",
        "        inputs, targets = batch['inputs'], batch['outputs']\n",
        "        inputs = inputs.transpose(0, 1)\n",
        "        targets = targets.transpose(0, 1)\n",
        "\n",
        "        output = self(inputs, targets)\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        y_hat = output.reshape(-1, output_dim)\n",
        "        y = targets.reshape(-1)\n",
        "\n",
        "        loss = self.loss(y_hat, y)\n",
        "        return loss\n",
        "\n",
        "      def training_step(self, batch, batch_idx):\n",
        "          loss = self.step(batch)\n",
        "          self.log('train_loss', loss, prog_bar = True)\n",
        "          return loss\n",
        "\n",
        "      def validation_step(self, batch, batch_idx):\n",
        "          loss = self.step(batch)\n",
        "          self.log('val_loss', loss, prog_bar = True)\n",
        "          return loss\n"
      ],
      "metadata": {
        "id": "49CQjkYrDQg7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq Models"
      ],
      "metadata": {
        "id": "Im389s_SCa_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(pl.LightningModule):\n",
        "    def __init__(self, tokenizer, model):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(**batch)\n",
        "        self.log('train_loss', outputs.loss, prog_bar=True, logger=True)\n",
        "        return outputs.loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(**batch)\n",
        "        self.log('val_loss', outputs.loss, prog_bar=True, logger=True)\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, dim=-1)\n",
        "        metrics = compute_seq2seq_metrics(preds, batch['labels'], self.tokenizer)\n",
        "        for key, value in metrics.items():\n",
        "            self.log(f'{key}', value, prog_bar=True, logger=True)\n",
        "\n",
        "        return outputs.loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(**batch)\n",
        "        self.log('test_loss', outputs.loss, prog_bar=True, logger=True)\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, dim=-1)\n",
        "        metrics = compute_seq2seq_metrics(preds, batch['labels'], self.tokenizer)\n",
        "\n",
        "        for key, value in metrics.items():\n",
        "            self.log(f'{key}', value, prog_bar=True, logger=True)\n",
        "        return outputs.loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler = {\n",
        "            'scheduler': get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=10000),\n",
        "            'name': 'learning_rate',\n",
        "            'interval': 'step',\n",
        "            'frequency': 1\n",
        "        }\n",
        "        return [optimizer], [scheduler]\n"
      ],
      "metadata": {
        "id": "kqRR8xEaCZmm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train!"
      ],
      "metadata": {
        "id": "UfMYpWq3EVEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if SEQ2SEQ:\n",
        "    model = Seq2Seq(tokenizer, model)\n",
        "    trainer = pl.Trainer(\n",
        "        precision='16-mixed',\n",
        "        max_epochs=MAX_EPOCHS,\n",
        "        enable_progress_bar=True,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=3)]\n",
        "    )\n",
        "    trainer.fit(model, train_dataloader, val_dataloader)\n",
        "    trainer.test(model, test_dataloader)\n",
        "elif RNN:\n",
        "    model = Recurrent()\n",
        "    trainer = pl.Trainer(max_epochs=MAX_EPOCHS)\n",
        "    trainer.fit(model, train_dataloader, val_dataloader)\n",
        "    print(evaluate(_device = \"cpu\", _print = True, _training= False))\n",
        "else:\n",
        "  model = FeedForward()\n",
        "  trainer = pl.Trainer(max_epochs=MAX_EPOCHS)\n",
        "  trainer.fit(model, train_dataloader, val_dataloader)\n",
        "  print(evaluate(_device = \"cpu\", _print = True, _training= False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842,
          "referenced_widgets": [
            "d3153f70318d461e97e4f667adcf7c03",
            "6eac34a5c9ef47b79b9fb6ea21bb36c9",
            "53ee2fd7914242a38453bb5c6c3c838c",
            "9033b989211046af8e0058649cd2a0d3",
            "6743ce227d0d4f2bbc1c5855f3b012fc",
            "a511cd9d32fa410da98ccbea63b401fa",
            "3b98af18eabb4731b476269fef4a8d28",
            "1421e2b303614245adf21d07b39d4261",
            "4ca1fde836904c23ae6fce60c9f5acb4",
            "42d41d7545a345808a26ff2879e48859",
            "d5afc7446ba64d26aee8162707ebd9f7",
            "ba6e9b8e9c494d6d982a23abc9bb37e8",
            "4cc0bd9f56994ea1af6aa217f68cffb7",
            "06718797fb694bc3821446914a6cec6d",
            "f1a9cda092ab4ccfa05b373904047932",
            "ed04d22b43024309b75aa046e16bab84",
            "88eebae2fc1148f4a2076b6e6a56d531",
            "1ee05b56eb8f4d5583619f68ee0c4b4c",
            "cd954e43c0cf4f4db4d6a2aa1f64c05c",
            "6f887261880746e28595248b343cb6b7",
            "1e4e5faa33124825a5fda66e9d1e5846",
            "81b83b955635466b9f7de8727b2ead1e"
          ]
        },
        "id": "Czx7bfJJEV-y",
        "outputId": "30f1ac02-507f-4ac9-a337-d2bc1088e8f2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name        | Type             | Params\n",
            "-------------------------------------------------\n",
            "0 | embedding   | Embedding        | 2.4 K \n",
            "1 | encoder_rnn | RNN              | 1.9 M \n",
            "2 | decoder_rnn | RNN              | 1.9 M \n",
            "3 | dropout     | Dropout          | 0     \n",
            "4 | linear      | Linear           | 9.7 K \n",
            "5 | criterion   | CrossEntropyLoss | 0     \n",
            "6 | loss        | CrossEntropyLoss | 0     \n",
            "-------------------------------------------------\n",
            "3.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.8 M     Total params\n",
            "15.286    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3153f70318d461e97e4f667adcf7c03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba6e9b8e9c494d6d982a23abc9bb37e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a051c87985a1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_print\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_training\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-b5c13716562b>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(_device, _print, _training)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-438baafd1e64>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, target, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2235\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ]
    }
  ]
}