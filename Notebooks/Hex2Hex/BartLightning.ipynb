{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets\n",
    "! pip install -U transformers\n",
    "! pip install -U accelerate\n",
    "! pip install evaluate\n",
    "! pip install wandb\n",
    "! pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "!wandb login 532eb90ecf4aa93d56a353a11b3b74c253d882cb\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"Seq2SeqZip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainingArguments\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "SEED = 999\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/hexadecimalzip/randomized_shorthex2hex.csv')\n",
    "df = df[:8000]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['deflate_hex'] = [elem + \"</s>\" for elem in df['deflate_hex']]      \n",
    "df['text_hex'] = [elem + \"</s>\" for elem in df['text_hex']]\n",
    "df['text'] = [elem + \"</s>\" for elem in df['text']]\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds_train_test = ds.train_test_split(test_size=0.2, seed=SEED)\n",
    "ds_test_dev = ds_train_test['test'].train_test_split(test_size=0.5, seed=SEED)\n",
    "ds_splits = DatasetDict({\n",
    "    'train': ds_train_test['train'],\n",
    "    'valid': ds_test_dev['train'],\n",
    "    'test': ds_test_dev['test']\n",
    "})\n",
    "\n",
    "print(ds_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSeq2SeqWithPadding:\n",
    "    tokenizer: BartTokenizer\n",
    "\n",
    "    def __call__(self, dataset_elements) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        # collect the input and output sequences\n",
    "        input_text = [de[\"text_hex\"] for de in dataset_elements]\n",
    "        output_text = [de[\"deflate_hex\"] for de in dataset_elements]\n",
    "\n",
    "        # tokenize both sequences in batch so that it will be much faster!\n",
    "        input_features = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",  # output directly tensors\n",
    "            padding=True, # add the padding on each sequence if needed\n",
    "            truncation=True # If the input sequence is too long, truncate it\n",
    "        )\n",
    "\n",
    "        output_features = self.tokenizer(\n",
    "            output_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )[\"input_ids\"]  # here we only need the input_ids (output actually)\n",
    "\n",
    "        output_features[output_features==self.tokenizer.pad_token_id] = -100 # cross entropy ignore index\n",
    "\n",
    "        # This is the only parameters we need for the forward pass\n",
    "        # to understand why, take a look to the BartForConditionalGeneration.forward method signature.\n",
    "        batch = {\n",
    "            \"input_ids\": input_features[\"input_ids\"],\n",
    "            \"attention_mask\": input_features[\"attention_mask\"],\n",
    "            \"labels\": output_features,\n",
    "        }\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSeq2SeqWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    for lab in labels:\n",
    "        for i in range(len(lab)):\n",
    "            if (lab[i] == -100):\n",
    "                lab[i] = tokenizer.pad_token_id\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    assert len(decoded_preds) == len(decoded_labels)\n",
    "    \n",
    "    results = []\n",
    "    count_unzippable = 0\n",
    "    for i in range(len(decoded_preds)):\n",
    "        distance = edit_distance(decoded_preds[i], decoded_labels[i])\n",
    "        results.append(distance)\n",
    "        if distance == 0:\n",
    "            count_unzippable += 1\n",
    "    \n",
    "    avg_distance = np.mean(results)\n",
    "    #print(f\"Avg Distance = {avg_distance}\")\n",
    "    \n",
    "    result_dict = {\"average_edit_distance\": avg_distance, \"count_unzippable\": count_unzippable}\n",
    "    #wandb.log(result_dict)\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Seq2Seq(pl.LightningModule):\n",
    "    def __init__(self, tokenizer, model, data_collator, training_args, compute_metrics):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.data_collator = data_collator\n",
    "        self.training_args = training_args\n",
    "        self.compute_metrics = compute_metrics\n",
    "        self.epoch_loss = []\n",
    "        self.valid_labels = []\n",
    "        self.valid_predictions = []\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss)\n",
    "        self.epoch_loss.append(loss.cpu().item())\n",
    "               \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        #print(f\"Epoch_loss = {self.epoch_loss}\")\n",
    "        print(f\"Loss = {np.mean(self.epoch_loss)}\")\n",
    "        self.epoch_loss = []\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):        \n",
    "        generated = self.model.generate(**batch, max_new_tokens=256)        \n",
    "        \n",
    "        for gen in generated:\n",
    "            self.valid_predictions.append(gen.cpu().tolist())\n",
    "        \n",
    "        for label in batch['labels']:\n",
    "            self.valid_labels.append(label.cpu().tolist())\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        res = compute_metrics(self.valid_predictions, self.valid_labels)\n",
    "        print(f\"Validation Edit distance = {res['average_edit_distance']}\")\n",
    "        print(f\"Validation Unzippable = {res['count_unzippable']}\")\n",
    "        return\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.training_args.learning_rate,\n",
    "            weight_decay=self.training_args.weight_decay\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=2e-4,\n",
    "    warmup_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=16,\n",
    "    generation_max_length=256,\n",
    "    eval_steps=400,\n",
    "    logging_steps=400,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    predict_with_generate=True,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    max_epochs = 10, \n",
    "    max_steps=training_args.max_steps,\n",
    "    accumulate_grad_batches=training_args.gradient_accumulation_steps,\n",
    "    val_check_interval=training_args.eval_steps,\n",
    "    logger=False,\n",
    "    callbacks=[pl.callbacks.ProgressBar()],\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(ds_splits[\"train\"], batch_size=training_args.per_device_train_batch_size, collate_fn=data_collator, num_workers=4)\n",
    "valid_dataloader = DataLoader(ds_splits[\"valid\"], batch_size=training_args.per_device_train_batch_size, collate_fn=data_collator, num_workers=4)\n",
    "\n",
    "seq2seq_model = Seq2Seq(tokenizer, model, data_collator, training_args, compute_metrics)\n",
    "trainer.fit(seq2seq_model, train_dataloader, valid_dataloader)\n",
    "trainer.test(seq2seq_model, DataLoader(ds_splits[\"test\"], batch_size=8, collate_fn=data_collator, num_workers=4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
