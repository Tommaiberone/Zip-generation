{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7711642,"sourceType":"datasetVersion","datasetId":4503096},{"sourceId":7834971,"sourceType":"datasetVersion","datasetId":4592427}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"05312497f8534c61a85270eb0bcd813c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06853c1004254f2fbeb1860bb04ba1ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3c4e805183b417385abf713b0e52925","placeholder":"​","style":"IPY_MODEL_fe177297c64d4964b19faf5d522ec801","value":" 1250/1250 [50:02&lt;00:00,  0.42it/s, v_num=6, train_loss_step=2.090, val_loss=2.160, train_loss_epoch=2.430]"}},"0f3d20ef311c42ff8812f003cfd1705e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11d9845ac8744742ad15a8041a589fb0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"1acfa5ed32bc4f97ad36ac3a57289fc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad4050eb5cd44c22a87639f91918ad81","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4999f7de3cb64aaab9f91a52a992d7d7","value":2}},"1d560b532bc445f9961f20e0b69ac60e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f6b0f71e8d14afbb1ab1c051cf4b08f","max":157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_545d2a9b94b5448f9a086668f2c723f4","value":157}},"2ce38747fdda48cc96951d17f3124eab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2e0c1c1d258f44b38e723fe85e834460":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43da7be4a80e4cefabf8935029a56f9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"460f6681c5ec4c9abb35560df6957135":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc7d28764cbf4f1694edda359360cbc7","placeholder":"​","style":"IPY_MODEL_da17fa8f5eb04abdb13794c977bce1d2","value":"Validation DataLoader 0: 100%"}},"47797ff7f45b49409580cb55e5443ce5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"4999f7de3cb64aaab9f91a52a992d7d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d7eb77d852c4fa29ae259eb276d8b38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c756ee5b81443c8a7c1b2e5aed518c6","placeholder":"​","style":"IPY_MODEL_bded4ff7a6fa45b9ad573cd04ddfb2e8","value":"Sanity Checking DataLoader 0: 100%"}},"50adee0f4c944f0d927186554a8e80fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf2f6fbf9f234ef1b3f20b93f485607c","IPY_MODEL_8cd2ec15bb3640fd963ac02dc6eb17af","IPY_MODEL_06853c1004254f2fbeb1860bb04ba1ed"],"layout":"IPY_MODEL_11d9845ac8744742ad15a8041a589fb0"}},"545d2a9b94b5448f9a086668f2c723f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55c99248c355463094e62d58de4191f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"571b901acd2b459c87cacac4e940a2ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58053cf21023434aa5fa3d51d4859a10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bf7ec0735dd47a3b1a6f7e5b765487e","placeholder":"​","style":"IPY_MODEL_43da7be4a80e4cefabf8935029a56f9e","value":"Validation DataLoader 0: 100%"}},"5f3a8c4dde17429a9bc85aaf022d874e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f6b0f71e8d14afbb1ab1c051cf4b08f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61dcffbd65ab493f9566cc26e08ce171":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63ca944b423f4dc4b198d03ddcb78147":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c756ee5b81443c8a7c1b2e5aed518c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d81b576c1e84d5681f53d0acd289fd7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7622403da97644c7bce4b44b6ca9d19f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ebc755ffc054d89bda7dcdce8115db6","placeholder":"​","style":"IPY_MODEL_bfea93d13a9d4ae08e17dd80f26e0da4","value":" 2/2 [00:01&lt;00:00,  1.14it/s]"}},"7a0940bb48f84761bb8b8d8cd224f4cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_947002135e3c4de7a7581ceb3b408cda","IPY_MODEL_1d560b532bc445f9961f20e0b69ac60e","IPY_MODEL_df3d04a295164c1d8d533793dc7d2829"],"layout":"IPY_MODEL_55c99248c355463094e62d58de4191f7"}},"7be410ae0f9b4e07be1cd1948b6fc3a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_571b901acd2b459c87cacac4e940a2ce","max":157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d20e7f00aeab409384472fa30d758fe5","value":157}},"8ad75655f08a47e8b37dca10d8aefbc0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8cd2ec15bb3640fd963ac02dc6eb17af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_05312497f8534c61a85270eb0bcd813c","max":1250,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ce38747fdda48cc96951d17f3124eab","value":1250}},"907ee8f31170492aa548dd5f8a1983e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93a5d3444f34409499157049e8018586":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61dcffbd65ab493f9566cc26e08ce171","placeholder":"​","style":"IPY_MODEL_cbc3c6b1dbf04e55b05f21e9e0975d27","value":" 157/157 [01:53&lt;00:00,  1.38it/s]"}},"947002135e3c4de7a7581ceb3b408cda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf2cd551a1554a9e90d09545eff23d23","placeholder":"​","style":"IPY_MODEL_f447384535624566befaaf9dcf8e7776","value":"Validation DataLoader 0: 100%"}},"9bf7ec0735dd47a3b1a6f7e5b765487e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ebc755ffc054d89bda7dcdce8115db6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a309f1e108ec45c2823a2dfb48b3f3cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_460f6681c5ec4c9abb35560df6957135","IPY_MODEL_ae7ee984a0d041ac9f066275868e7a26","IPY_MODEL_c9af168e3b524d8ab679008b3d9c5b59"],"layout":"IPY_MODEL_47797ff7f45b49409580cb55e5443ce5"}},"ad4050eb5cd44c22a87639f91918ad81":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae7ee984a0d041ac9f066275868e7a26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d81b576c1e84d5681f53d0acd289fd7","max":157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_907ee8f31170492aa548dd5f8a1983e3","value":157}},"b30fbe68d5ee47c8befe352d7e8cdfcc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"ba625b5ab6c641dfa9112594a470c49b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4d7eb77d852c4fa29ae259eb276d8b38","IPY_MODEL_1acfa5ed32bc4f97ad36ac3a57289fc6","IPY_MODEL_7622403da97644c7bce4b44b6ca9d19f"],"layout":"IPY_MODEL_b30fbe68d5ee47c8befe352d7e8cdfcc"}},"bd6fdb894f21489694ebeb117abe27f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bded4ff7a6fa45b9ad573cd04ddfb2e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf2cd551a1554a9e90d09545eff23d23":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf2f6fbf9f234ef1b3f20b93f485607c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f3a8c4dde17429a9bc85aaf022d874e","placeholder":"​","style":"IPY_MODEL_8ad75655f08a47e8b37dca10d8aefbc0","value":"Epoch 2: 100%"}},"bfea93d13a9d4ae08e17dd80f26e0da4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9af168e3b524d8ab679008b3d9c5b59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f3d20ef311c42ff8812f003cfd1705e","placeholder":"​","style":"IPY_MODEL_bd6fdb894f21489694ebeb117abe27f9","value":" 157/157 [01:54&lt;00:00,  1.37it/s]"}},"cbc3c6b1dbf04e55b05f21e9e0975d27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc7d28764cbf4f1694edda359360cbc7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d20e7f00aeab409384472fa30d758fe5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da17fa8f5eb04abdb13794c977bce1d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df3d04a295164c1d8d533793dc7d2829":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63ca944b423f4dc4b198d03ddcb78147","placeholder":"​","style":"IPY_MODEL_2e0c1c1d258f44b38e723fe85e834460","value":" 157/157 [01:54&lt;00:00,  1.37it/s]"}},"e3c4e805183b417385abf713b0e52925":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f447384535624566befaaf9dcf8e7776":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8535fa4def7498ead61fb2bdb066f44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_58053cf21023434aa5fa3d51d4859a10","IPY_MODEL_7be410ae0f9b4e07be1cd1948b6fc3a9","IPY_MODEL_93a5d3444f34409499157049e8018586"],"layout":"IPY_MODEL_fe46ffd717cb4f199d50f91bfc81aaba"}},"fe177297c64d4964b19faf5d522ec801":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe46ffd717cb4f199d50f91bfc81aaba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports and installation\n","metadata":{"id":"JkKtT4GK-7y1"}},{"cell_type":"code","source":"%%capture\n!pip install datasets transformers accelerate evaluate wandb nltk pandas","metadata":{"id":"woNI1XG6H-Bb","execution":{"iopub.status.busy":"2024-03-14T14:12:47.102006Z","iopub.execute_input":"2024-03-14T14:12:47.102895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom torch.utils.data import DataLoader\nimport random\nfrom dataclasses import dataclass\nimport numpy as np\nfrom transformers import BartTokenizer, BartForConditionalGeneration, T5ForConditionalGeneration, AutoTokenizer\nimport pytorch_lightning as L\n\nBATCH_SIZE = 128\nSEED = 124\n\n#DATA USED\nSHORT = False\nRANDOMIZED_SHORT = True\nMEDIUM = False\n\nMAX_SEQ_LEN = 512 if MEDIUM else 256\n\n#MODEL USED\nFEEDFORWARD = False\nFEEDFORWARD_WITH_ATTENTION = True\nCONV1D = False\nRNN = False\nSEQ2SEQ = False\n\n#MODEL CHOICES FOR SEQ2SEQ: bart-base, bart-large, t5-base\nMODEL = \"bart-base\"\n\n#RNN MODELS AND HYPERPARAMETERS\nBIDIRECTIONAL = False\nRNN_TYPE = 'RNN'  # Options: 'LSTM', 'GRU', 'RNN'\n\n#HYPERPARAMETERS\nEMBED_DIM = 128\nHIDDEN_DIM = 512\nLEARNING_RATE = 5e-4\nDROPOUT_RATE = 0.5\nNUM_HEADS = 4\nNUM_LAYERS = 4\nWEIGHT_DECAY = 0.01\nMAX_EPOCHS = 5\n\ntoken2id = {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6, \"7\": 7, \"8\": 8, \"9\": 9, \"a\": 10, \"b\": 11, \"c\": 12, \"d\": 13, \"e\": 14, \"f\": 15, \"P\":16, \"S\": 17, \"E\":18 }\nOUTPUT_DIM = len(token2id)\n\ntorch.manual_seed(SEED)\nL.seed_everything(SEED)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBg7g-c1A7kR","outputId":"15238e60-c215-42d7-d027-ba7a055929db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"afxIsaS-8AhR","outputId":"8a666d5e-2ef7-4e46-a1f7-16764ae80874","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{"id":"mwbZbYZN_05Z"}},{"cell_type":"code","source":"if SHORT:\n  df = pd.read_csv('/kaggle/input/full-dataset/shorthex2hex.csv')\nelif RANDOMIZED_SHORT:\n  df = pd.read_csv('/kaggle/input/full-dataset/randomized_shorthex2hex.csv')\nelif MEDIUM:\n  df = pd.read_csv('/kaggle/input/full-dataset/mediumhex2hex.csv')\n\n\ndf = df[:40960]","metadata":{"id":"f-sQt-E_Arl2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RNN:\n  df['text_hex'] = 'S' + df['text_hex'] + 'E'\n  df['deflate_hex'] = 'S' + df['deflate_hex'] + 'E'\n\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"aT-D7npnp_3p","outputId":"e9fdbf2f-53bb-4023-8f53-49225174e542","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of using the standard \\<EOS> and \\<SOS> tags we're using the letter S and E since they are not present in the vocabulary","metadata":{"id":"bVUV1WDuvi7G"}},{"cell_type":"code","source":"if SEQ2SEQ:\n    df = df[:15000]\n    df[['deflate_hex', 'text_hex', 'text']] += \"</s>\"\n\nds = Dataset.from_pandas(df)\nds_train_test = ds.train_test_split(test_size=0.2, seed=SEED)\nds_test_dev = ds_train_test['test'].train_test_split(test_size=0.5, seed=SEED)\nds_splits = DatasetDict({\n    'train': ds_train_test['train'],\n    'valid': ds_test_dev['train'],\n    'test': ds_test_dev['test']\n})","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LE9OmLvsIdlf","outputId":"aee3fbdd-5a75-40bf-aa5c-584822f22963","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data tokenization","metadata":{"id":"YBdD8nIBrL30"}},{"cell_type":"code","source":"def create_id2token_vocab(token_to_id):\n    id2token = {}\n    for token, id in token_to_id.items():\n        id2token[id] = token\n\n    return id2token\n\nid2token = create_id2token_vocab(token2id)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I4dMwVcvySQQ","outputId":"165e43d7-2645-4ac4-970d-297f56727d8c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SEQ2SEQ:\n    if (MODEL == \"bart-base\"):\n        tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n\n    elif (MODEL == \"bart-large\"):\n        tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n        model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Corrected collate_fn function\ndef collate_fn(batch):\n\n    if FEEDFORWARD or FEEDFORWARD_WITH_ATTENTION or CONV1D:\n\n        texts = [elem['text_hex'] for elem in batch]\n        encoded_hexs = [[token2id[char] for char in text] for text in texts]\n\n        outputs = [elem['deflate_hex'] for elem in batch]\n        encoded_outputs = [[token2id[char] for char in output] for output in outputs]\n\n        # # Pad the sequences to MAX_SEQ_LEN chars with the padding token\n        padded_hex = [torch.Tensor(encoded_hex + [token2id[\"P\"]] * (MAX_SEQ_LEN - len(encoded_hex))) for encoded_hex in encoded_hexs]\n        padded_outputs = [torch.Tensor(encoded_output + [token2id[\"P\"]] * (MAX_SEQ_LEN - len(encoded_output))) for encoded_output in encoded_outputs]\n\n        # # Stack the sequences\n        padded_hex = torch.stack(padded_hex).long()\n        padded_outputs = torch.stack(padded_outputs).long()\n\n\n        return {\n            'inputs': padded_hex,\n            'outputs': padded_outputs\n        }\n    \n    elif RNN:\n        \n        def pad_sequences(sequences, maxlen, value=token2id['P']):\n            padded_sequences = []\n            for sequence in sequences:\n                padded_sequence = sequence[:maxlen]\n                padded_sequence.extend([value] * (maxlen - len(padded_sequence)))\n\n                padded_sequence = sequence +  [value] * (maxlen - len(sequence))\n                padded_sequences.append(padded_sequence)\n\n            return padded_sequences\n\n\n        texts = [elem['text_hex'] for elem in batch]\n        encoded_hex = [[token2id[x] for x in hex] for hex in texts]\n\n\n        outputs = [elem['deflate_hex'] for elem in batch]\n        encoded_outputs = [[token2id[x] for x in hex] for hex in outputs]\n\n\n        maxlen = 0\n        for seq in encoded_hex:\n            if len(seq) > maxlen:\n                maxlen = len(seq)\n        for seq in encoded_outputs:\n            if len(seq) > maxlen:\n                maxlen = len(seq)\n\n        padded_encoded_hex = pad_sequences(encoded_hex, maxlen)\n        padded_encoded_outputs = pad_sequences(encoded_outputs, maxlen)\n\n\n        return {\n            'inputs': torch.tensor(padded_encoded_hex),\n            \"outputs\": torch.tensor(padded_encoded_outputs)\n        }\n    \n    elif SEQ2SEQ:\n        inputs = [x[\"text_hex\"] for x in batch]\n        outputs = [x[\"deflate_hex\"] for x in batch]\n        input_features = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SEQ_LEN)\n        output_features = tokenizer(outputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SEQ_LEN)[\"input_ids\"]\n        output_features[output_features == tokenizer.pad_token_id] = -100\n        return {\"input_ids\": input_features[\"input_ids\"], \"attention_mask\": input_features[\"attention_mask\"], \"labels\": output_features}\n","metadata":{"id":"VF82QijJt4nw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(ds_splits['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers = 3)\nval_dataloader = DataLoader(ds_splits['valid'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers = 3)\ntest_dataloader = DataLoader(ds_splits['test'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers = 3)\n\ntorch.set_printoptions(profile=\"full\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.metrics.distance import edit_distance\n\ndef decode_output(output):\n    return ''.join([id2token[int(id)] for id in output])\n\ndef decode_input(input):\n    return ''.join([id2token[int(id)] for id in input])\n\ndef compute_seq2seq_metrics(preds, labels, tokenizer):\n    # Ensure labels with -100 are replaced by pad_token_id\n    labels = torch.where(labels == -100, tokenizer.pad_token_id, labels)\n\n    # Convert tensors to lists for decoding if they're not already in CPU\n    if torch.is_tensor(preds):\n        preds = preds.detach().cpu().tolist()\n    if torch.is_tensor(labels):\n        labels = labels.detach().cpu().tolist()\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    distances = [edit_distance(p, l) for p, l in zip(decoded_preds, decoded_labels)]\n    avg_distance = np.mean(distances)\n    count_unzippable = distances.count(0)\n\n    return {\"average_edit_distance\": avg_distance, \"count_unzippable\": count_unzippable}\n\ndef evaluate(_device, _print, _training):\n\n    model.eval()\n    total_distance = 0\n    total = 0\n\n    distances_list = []\n\n\n    for batch in test_dataloader:\n            \n        if FEEDFORWARD or FEEDFORWARD_WITH_ATTENTION or CONV1D:\n            x = batch[\"inputs\"].to(_device)\n            y = batch[\"outputs\"].to(_device)\n\n            y_hat = model(x, y)\n            y_hat = torch.argmax(y_hat, dim=-1)\n\n            output = decode_output(y[0])\n            output_hat = decode_output(y_hat[0])\n\n            output = [x for x in output if x != \"P\"]\n            output_hat = [x for x in output_hat if x != \"P\"]\n            \n            distance = edit_distance(output, output_hat)\n\n        elif RNN:\n            x = batch[\"inputs\"].transpose(0,1).to(_device)\n            y = batch[\"outputs\"].transpose(0,1).to(_device)\n\n            y_hat = model(x, y)\n            y_hat = torch.argmax(y_hat, dim=-1)\n            \n            y = y.transpose(0,1)\n            y_hat = y_hat.transpose(0,1)\n            \n            assert len(y) == len(y_hat)\n            \n            for i in range(len(y)):\n                output = decode_output(y[i])\n                output_hat = decode_output(y_hat[i])\n\n                output = [x for x in output if x != \"P\"]\n                output_hat = [x for x in output_hat if x != \"P\"]\n\n                first_eos_index = 0\n                for i in range(len(output_hat)):\n                    if output_hat[i] == \"E\":\n                        first_eos_index = i\n                        break\n\n                # REMOVE START OF SEQUENCE TOKEN\n                output = output[1:]\n                output_hat = output_hat[1:first_eos_index]\n                distance = edit_distance(output, output_hat)\n                distances_list.append(distance)\n\n        if _print:\n            print(f\"output = {output}\")\n            print(f\"output_hat = {output_hat}\")\n\n        total_distance += distance\n        total += 1\n\n        if distance == 0:\n            print(f\"DISTANCE = 0!\")\n            print(f\"output = {output}\")\n            print(f\"output_hat = {output_hat}\")\n\n        if _training:\n            return total_distance/total\n\n    return (total_distance/total, distances_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR\nfrom pytorch_lightning.callbacks import EarlyStopping\nfrom transformers import get_linear_schedule_with_warmup\n\nclass FeedForward(pl.LightningModule):\n\n    def __init__(self, input_dim=MAX_SEQ_LEN, embed_dim = EMBED_DIM, hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM, learning_rate=LEARNING_RATE,\n                 dropout_rate=DROPOUT_RATE, bidirectional=BIDIRECTIONAL, num_layers=NUM_LAYERS, optimizer_type=AdamW, scheduler_type=StepLR,\n                 scheduler_step_size=5, scheduler_gamma=0.1):\n        super().__init__()\n        self.save_hyperparameters()\n\n        if FEEDFORWARD or FEEDFORWARD_WITH_ATTENTION:\n            self.embed = nn.Embedding(input_dim, embed_dim)\n            self.positional_embeddings = nn.Parameter(torch.zeros(BATCH_SIZE, input_dim, embed_dim))\n            nn.init.normal_(self.positional_embeddings, mean=0, std=embed_dim ** -0.5)  # Initialize positional embeddings\n            self.self_attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=NUM_HEADS, dropout=dropout_rate, batch_first = True)\n            self.fc1 = nn.Linear(embed_dim, hidden_dim)\n            self.norm1 = nn.LayerNorm(hidden_dim)\n            self.dropout = nn.Dropout(dropout_rate)\n            self.fc2 = nn.Linear(hidden_dim, hidden_dim//2)\n            self.norm2 = nn.LayerNorm(hidden_dim//2)\n            self.fc3 = nn.Linear(hidden_dim//2, output_dim)\n\n        elif CONV1D:\n\n            # Embedding layer to transform dictionary indices into dense vectors\n            self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=embed_dim)\n\n            # Convolutional layers\n            self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=3, padding=1)\n\n            # Fully connected layers for classification\n            self.fc1 = nn.Linear(embed_dim, embed_dim)  \n            self.fc2 = nn.Linear(embed_dim, output_dim)\n\n            # Hyperparameters\n            self.learning_rate = learning_rate\n\n        elif RNN:\n            self.rnn_type = RNN_TYPE\n            self.embedding = nn.Embedding(output_dim, embed_dim, padding_idx=token2id['P'])\n            \n            if self.rnn_type == 'LSTM':\n                rnn_cell = nn.LSTM\n            elif self.rnn_type == 'GRU':\n                rnn_cell = nn.GRU\n            else:  # Default to RNN if neither LSTM nor GRU is selected\n                rnn_cell = nn.RNN\n            \n            self.encoder_rnn = rnn_cell(embed_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_rate if num_layers > 1 else 0)\n            self.decoder_rnn = rnn_cell(embed_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_rate if num_layers > 1 else 0)\n            \n            self.dropout = nn.Dropout(dropout_rate)\n            self.output_dim = output_dim\n            self.linear = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n            self.criterion = nn.CrossEntropyLoss()\n            \n        self.loss = nn.CrossEntropyLoss()\n\n    def forward(self, x, target, teacher_forcing_ratio=0.5):\n\n        if FEEDFORWARD or FEEDFORWARD_WITH_ATTENTION:\n\n            # Embedding\n            x = self.embed(x)  # Shape: [Batch, Seq_len, Embed_dim]\n\n            if (FEEDFORWARD_WITH_ATTENTION):   \n                \n                # Add positional embeddings\n                positions = self.positional_embeddings\n                x = x + positions\n                \n                # Self-attention\n                attn_output, _ = self.self_attention(x, x, x)\n            \n                x = torch.relu(self.norm1(self.fc1(attn_output)))\n\n            else:\n                x = torch.relu(self.norm1(self.fc1(x)))\n                \n            x = self.dropout(x)\n            x = torch.relu(self.norm2(self.fc2(x)))\n            x = self.fc3(x)\n            \n            return x\n        \n        elif CONV1D:\n            \n            # Embedding layer\n            x = self.embedding(x) \n\n            # Transpose from (batch_size, sequence_length, embedding_dim) to (batch_size, embedding_dim, sequence_length)\n            x = x.permute(0, 2, 1)\n\n            x = torch.relu(self.conv1(x))\n\n            x = x.permute(0, 2, 1)\n\n            x = torch.relu(self.fc1(x))\n\n            x = self.fc2(x)\n            \n            return x\n        \n        elif RNN:\n            target_len = target.shape[0]\n            batch_size = target.shape[1]\n            target_vocab_size = self.output_dim\n\n            outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n\n            x = self.dropout(self.embedding(x))\n            rnn_output, h = self.encoder_rnn(x)\n\n            x = target[0]\n            for t in range(1, target_len):\n                x = self.dropout(self.embedding(x.unsqueeze(0)))\n                out, h = self.decoder_rnn(x, h if self.rnn_type in ['LSTM', 'GRU'] else None)\n                predictions = self.linear(out)\n                predictions = predictions.squeeze(0)\n                outputs[t] = predictions\n                pred = predictions.argmax(1)\n                x = target[t] if random.random() < teacher_forcing_ratio else pred\n\n            return outputs\n\n    def configure_optimizers(self):\n        optimizer = self.hparams.optimizer_type(self.parameters(), lr=self.hparams.learning_rate)\n        scheduler = self.hparams.scheduler_type(optimizer, step_size=self.hparams.scheduler_step_size, gamma=self.hparams.scheduler_gamma)\n        return [optimizer], [scheduler]\n\n    def step(self, batch):\n        if (FEEDFORWARD or FEEDFORWARD_WITH_ATTENTION or CONV1D):\n            x = batch[\"inputs\"]\n            y = batch[\"outputs\"]\n            y = y.view(y.shape[0] * y.shape[1])\n            y_hat = self(x, y)\n            y_hat = y_hat.view(y_hat.shape[0] * y_hat.shape[1], y_hat.shape[2])\n\n        elif RNN:\n            inputs, targets = batch['inputs'], batch['outputs']\n            inputs = inputs.transpose(0, 1)\n            targets = targets.transpose(0, 1)\n\n            output = self(inputs, targets)\n            output_dim = output.shape[-1]\n\n            y_hat = output.reshape(-1, output_dim)\n            y = targets.reshape(-1)\n\n        loss = self.loss(y_hat, y)\n        return loss\n\n    def training_step(self, batch, batch_idx):  \n        loss = self.step(batch)\n        self.log('train_loss', loss, prog_bar = True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        loss = self.step(batch)\n        self.log('val_loss', loss, prog_bar = True)\n        return loss\n    \n\nclass Seq2Seq(pl.LightningModule):\n    def __init__(self, tokenizer, model):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.model = model\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n\n    def training_step(self, batch, batch_idx):\n        outputs = self.forward(**batch)\n        self.log('train_loss', outputs.loss, prog_bar=True, logger=True)\n        return outputs.loss\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self.forward(**batch)\n        self.log('val_loss', outputs.loss, prog_bar=True, logger=True)\n\n        preds = torch.argmax(outputs.logits, dim=-1)\n        metrics = compute_seq2seq_metrics(preds, batch['labels'], self.tokenizer)\n        for key, value in metrics.items():\n            self.log(f'{key}', value, prog_bar=True, logger=True)\n\n        return outputs.loss\n\n    def test_step(self, batch, batch_idx):\n        outputs = self.forward(**batch)\n        self.log('test_loss', outputs.loss, prog_bar=True, logger=True)\n        \n        preds = torch.argmax(outputs.logits, dim=-1)\n        metrics = compute_seq2seq_metrics(preds, batch['labels'], self.tokenizer)\n        \n        for key, value in metrics.items():\n            self.log(f'{key}', value, prog_bar=True, logger=True)\n        return outputs.loss\n\n    def configure_optimizers(self):\n        optimizer = AdamW(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        scheduler = {\n            'scheduler': get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=10000),\n            'name': 'learning_rate',\n            'interval': 'step',\n            'frequency': 1\n        }\n        return [optimizer], [scheduler]\n\n\nif SEQ2SEQ:\n    model = Seq2Seq(tokenizer, model)\n    trainer = pl.Trainer(\n        precision='16-mixed',\n        max_epochs=MAX_EPOCHS,\n        enable_progress_bar=True,\n        callbacks=[EarlyStopping(monitor='val_loss', patience=3)]\n    )\n    trainer.fit(model, train_dataloader, val_dataloader)\n    trainer.test(model, test_dataloader)\nelse:\n    model = FeedForward()\n    trainer = pl.Trainer(max_epochs=MAX_EPOCHS)\n    trainer.fit(model, train_dataloader, val_dataloader)\n    print(evaluate(_device = \"cpu\", _print = True, _training= False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}