{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch pandas lightning trl\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 999\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df['text'] before: 0        [0, 3762, 9, 5, 97, 34910, 34, 2801, 14, 71, 2...\n",
      "1        [0, 250, 4613, 410, 931, 4, 28696, 3809, 1589,...\n",
      "2        [0, 100, 802, 42, 21, 10, 4613, 169, 7, 1930, ...\n",
      "3        [0, 34480, 89, 18, 10, 284, 147, 10, 410, 2143...\n",
      "4        [0, 28970, 1334, 21129, 118, 18, 22, 16587, 11...\n",
      "                               ...                        \n",
      "49995    [0, 100, 802, 42, 1569, 222, 10, 159, 235, 205...\n",
      "49996    [0, 26954, 6197, 6, 1099, 6054, 6, 1099, 3501,...\n",
      "49997    [0, 100, 524, 10, 4019, 5850, 11, 2242, 4306, ...\n",
      "49998    [0, 100, 437, 164, 7, 33, 7, 11967, 19, 5, 986...\n",
      "49999    [0, 3084, 65, 3352, 5, 2141, 20351, 4133, 7, 2...\n",
      "Name: text, Length: 50000, dtype: object\n",
      "text_min_len: 10\n",
      "df['deflate_hex'] before: 0        [0, 39413, 438, 288, 417, 3245, 23417, 242, 39...\n",
      "1        [0, 39413, 438, 26866, 7309, 698, 34836, 1922,...\n",
      "2        [0, 39413, 438, 288, 417, 398, 28690, 18616, 4...\n",
      "3        [0, 39413, 438, 306, 417, 398, 23219, 428, 288...\n",
      "4        [0, 39413, 438, 176, 417, 7309, 23417, 242, 29...\n",
      "                               ...                        \n",
      "49995    [0, 39413, 438, 996, 5046, 417, 18616, 438, 46...\n",
      "49996    [0, 39413, 438, 176, 417, 398, 16134, 3248, 41...\n",
      "49997    [0, 39413, 438, 34392, 438, 31132, 1043, 3103,...\n",
      "49998    [0, 39413, 438, 176, 417, 398, 102, 31132, 102...\n",
      "49999    [0, 39413, 438, 176, 417, 398, 428, 245, 428, ...\n",
      "Name: deflate_hex, Length: 50000, dtype: object\n",
      "hex_min_len: 46\n",
      "torch.Size([50000, 10])\n",
      "torch.Size([50000, 46])\n",
      "torch.Size([1024, 10])\n",
      "torch.Size([1024, 46])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../../Datasets/new_dataset_deflate.csv')\n",
    "\n",
    "# Tokenize the text column\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "df['text'] = df['text'].apply(lambda x: tokenizer.encode(x, truncation=True))\n",
    "df['deflate_hex'] = df['deflate_hex'].apply(lambda x: tokenizer.encode(x, truncation=True))\n",
    "\n",
    "# Function to calculate minimum length\n",
    "def calculate_minimum_length(column):\n",
    "    return min(len(seq) for seq in column)\n",
    "\n",
    "# Calculate average lengths\n",
    "text_min_len = calculate_minimum_length(df['text'])\n",
    "hex_min_len = calculate_minimum_length(df['deflate_hex'])\n",
    "\n",
    "# Function to pad and truncate sequences\n",
    "def truncate_sequences(sequences, target_length):\n",
    "    return [sequence[:target_length] for sequence in sequences] \n",
    "\n",
    "# Pad and truncate text sequences\n",
    "\n",
    "print(f\"df['text'] before: {df['text']}\")\n",
    "print(f\"text_min_len: {text_min_len}\")\n",
    "print(f\"df['deflate_hex'] before: {df['deflate_hex']}\")\n",
    "print(f\"hex_min_len: {hex_min_len}\")\n",
    "\n",
    "df['text'] = truncate_sequences(df['text'], text_min_len)\n",
    "df['deflate_hex'] = truncate_sequences(df['deflate_hex'], hex_min_len)\n",
    "\n",
    "# Convert to tensors\n",
    "text_tensor = torch.tensor(df['text'].tolist(), dtype=torch.float32)\n",
    "print(text_tensor.shape)\n",
    "\n",
    "hex_tensor = torch.tensor([list(map(int, list(bin_seq))) for bin_seq in df['deflate_hex']], dtype=torch.float32)\n",
    "print(hex_tensor.shape)\n",
    "\n",
    "train_size = int(0.8 * len(text_tensor))\n",
    "val_size = int(0.8 * train_size)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_text, train_hex = text_tensor[:train_size], hex_tensor[:train_size]\n",
    "val_text, val_hex = train_text[:val_size], train_hex[:val_size]\n",
    "test_text, test_hex = text_tensor[train_size:], hex_tensor[train_size:]\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "train_data = TensorDataset(train_text, train_hex)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_data = TensorDataset(val_text, val_hex)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(test_text, test_hex)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Test the data loaders\n",
    "for text, hex in train_loader:\n",
    "    print(text.shape)\n",
    "    print(hex.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]val_loss: 218759104.0\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 12.90it/s]val_loss: 221764976.0\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 40/40 [00:00<00:00, 56.89it/s]val_loss: 220630688.0\n",
      "val_loss: 219465952.0\n",
      "val_loss: 220290992.0\n",
      "val_loss: 218038768.0\n",
      "val_loss: 219814832.0\n",
      "val_loss: 219205728.0\n",
      "val_loss: 218368000.0\n",
      "val_loss: 216166176.0\n",
      "val_loss: 214549664.0\n",
      "val_loss: 220892816.0\n",
      "val_loss: 219742608.0\n",
      "val_loss: 218755792.0\n",
      "val_loss: 217150336.0\n",
      "val_loss: 220281728.0\n",
      "val_loss: 219545152.0\n",
      "val_loss: 221283328.0\n",
      "val_loss: 219296992.0\n",
      "val_loss: 216364656.0\n",
      "val_loss: 219938688.0\n",
      "val_loss: 218419696.0\n",
      "val_loss: 221002464.0\n",
      "val_loss: 220014880.0\n",
      "val_loss: 217844672.0\n",
      "val_loss: 217194768.0\n",
      "val_loss: 219567776.0\n",
      "val_loss: 219486992.0\n",
      "val_loss: 221463696.0\n",
      "val_loss: 220031952.0\n",
      "val_loss: 218036032.0\n",
      "val_loss: 218882432.0\n",
      "val_loss: 224930464.0\n",
      "val_loss: 224104624.0\n",
      "Epoch 1: 100%|██████████| 40/40 [00:00<00:00, 58.95it/s]val_loss: 218003280.0\n",
      "val_loss: 218022560.0\n",
      "val_loss: 216780672.0\n",
      "val_loss: 218999008.0\n",
      "val_loss: 217580944.0\n",
      "val_loss: 218935328.0\n",
      "val_loss: 221127776.0\n",
      "val_loss: 219520608.0\n",
      "val_loss: 221742128.0\n",
      "val_loss: 219607584.0\n",
      "val_loss: 221013616.0\n",
      "val_loss: 219242752.0\n",
      "val_loss: 219665296.0\n",
      "val_loss: 218196640.0\n",
      "val_loss: 220134784.0\n",
      "val_loss: 216710864.0\n",
      "val_loss: 219394784.0\n",
      "val_loss: 223840816.0\n",
      "val_loss: 220762208.0\n",
      "val_loss: 218892112.0\n",
      "val_loss: 221646448.0\n",
      "val_loss: 218071440.0\n",
      "val_loss: 220491984.0\n",
      "val_loss: 215861696.0\n",
      "val_loss: 214848672.0\n",
      "val_loss: 219074336.0\n",
      "val_loss: 219054352.0\n",
      "val_loss: 219159824.0\n",
      "val_loss: 218939840.0\n",
      "val_loss: 217273568.0\n",
      "val_loss: 216858848.0\n",
      "val_loss: 216782336.0\n",
      "Epoch 2: 100%|██████████| 40/40 [00:00<00:00, 70.35it/s]val_loss: 219766192.0\n",
      "val_loss: 218019312.0\n",
      "val_loss: 217241296.0\n",
      "val_loss: 214382416.0\n",
      "val_loss: 218864240.0\n",
      "val_loss: 220262176.0\n",
      "val_loss: 218555968.0\n",
      "val_loss: 220292304.0\n",
      "val_loss: 219222224.0\n",
      "val_loss: 221081008.0\n",
      "val_loss: 218210128.0\n",
      "val_loss: 218765920.0\n",
      "val_loss: 216570256.0\n",
      "val_loss: 214877792.0\n",
      "val_loss: 221628416.0\n",
      "val_loss: 219047536.0\n",
      "val_loss: 220013232.0\n",
      "val_loss: 218708992.0\n",
      "val_loss: 219054976.0\n",
      "val_loss: 215221344.0\n",
      "val_loss: 218228720.0\n",
      "val_loss: 216310480.0\n",
      "val_loss: 215728960.0\n",
      "val_loss: 223309360.0\n",
      "val_loss: 219715760.0\n",
      "val_loss: 216184416.0\n",
      "val_loss: 220208848.0\n",
      "val_loss: 223518720.0\n",
      "val_loss: 218380064.0\n",
      "val_loss: 217947808.0\n",
      "val_loss: 220841680.0\n",
      "val_loss: 218755232.0\n",
      "Epoch 3: 100%|██████████| 40/40 [00:00<00:00, 59.83it/s]val_loss: 217394048.0\n",
      "val_loss: 217943424.0\n",
      "val_loss: 216087376.0\n",
      "val_loss: 220433072.0\n",
      "val_loss: 214822288.0\n",
      "val_loss: 219768208.0\n",
      "val_loss: 218995584.0\n",
      "val_loss: 216309136.0\n",
      "val_loss: 217051536.0\n",
      "val_loss: 214525312.0\n",
      "val_loss: 218536480.0\n",
      "val_loss: 217607536.0\n",
      "val_loss: 219809440.0\n",
      "val_loss: 218756832.0\n",
      "val_loss: 221025392.0\n",
      "val_loss: 217722752.0\n",
      "val_loss: 219533888.0\n",
      "val_loss: 217255584.0\n",
      "val_loss: 220084144.0\n",
      "val_loss: 219881840.0\n",
      "val_loss: 219269616.0\n",
      "val_loss: 215594560.0\n",
      "val_loss: 218730272.0\n",
      "val_loss: 219176944.0\n",
      "val_loss: 220589056.0\n",
      "val_loss: 221876176.0\n",
      "val_loss: 218014592.0\n",
      "val_loss: 216971536.0\n",
      "val_loss: 220828096.0\n",
      "val_loss: 219217344.0\n",
      "val_loss: 218408272.0\n",
      "val_loss: 215779648.0\n",
      "Epoch 4: 100%|██████████| 40/40 [00:00<00:00, 70.60it/s]val_loss: 219623296.0\n",
      "val_loss: 213803888.0\n",
      "val_loss: 218765584.0\n",
      "val_loss: 221425600.0\n",
      "val_loss: 218099776.0\n",
      "val_loss: 220057552.0\n",
      "val_loss: 217091008.0\n",
      "val_loss: 217520176.0\n",
      "val_loss: 218314672.0\n",
      "val_loss: 218815824.0\n",
      "val_loss: 216264112.0\n",
      "val_loss: 215626352.0\n",
      "val_loss: 215797824.0\n",
      "val_loss: 218971984.0\n",
      "val_loss: 216673648.0\n",
      "val_loss: 218117264.0\n",
      "val_loss: 220831696.0\n",
      "val_loss: 217271664.0\n",
      "val_loss: 216242320.0\n",
      "val_loss: 216574848.0\n",
      "val_loss: 221225856.0\n",
      "val_loss: 215181584.0\n",
      "val_loss: 220753168.0\n",
      "val_loss: 217310880.0\n",
      "val_loss: 220201456.0\n",
      "val_loss: 220608512.0\n",
      "val_loss: 219464288.0\n",
      "val_loss: 213891408.0\n",
      "val_loss: 219962032.0\n",
      "val_loss: 215530384.0\n",
      "val_loss: 221859888.0\n",
      "val_loss: 222721568.0\n",
      "Epoch 5: 100%|██████████| 40/40 [00:00<00:00, 58.47it/s]val_loss: 217130160.0\n",
      "val_loss: 217023040.0\n",
      "val_loss: 214106880.0\n",
      "val_loss: 217053456.0\n",
      "val_loss: 221488928.0\n",
      "val_loss: 218797856.0\n",
      "val_loss: 217199792.0\n",
      "val_loss: 216702976.0\n",
      "val_loss: 214982528.0\n",
      "val_loss: 218642752.0\n",
      "val_loss: 219795104.0\n",
      "val_loss: 220204800.0\n",
      "val_loss: 218027776.0\n",
      "val_loss: 217080608.0\n",
      "val_loss: 214347888.0\n",
      "val_loss: 217289552.0\n",
      "val_loss: 217881024.0\n",
      "val_loss: 220757664.0\n",
      "val_loss: 219348016.0\n",
      "val_loss: 220602992.0\n",
      "val_loss: 219300688.0\n",
      "val_loss: 210966976.0\n",
      "val_loss: 219427440.0\n",
      "val_loss: 218025056.0\n",
      "val_loss: 215918064.0\n",
      "val_loss: 219590176.0\n",
      "val_loss: 216416080.0\n",
      "val_loss: 216710864.0\n",
      "val_loss: 219428464.0\n",
      "val_loss: 218995264.0\n",
      "val_loss: 221263872.0\n",
      "val_loss: 217952656.0\n",
      "Epoch 6: 100%|██████████| 40/40 [00:00<00:00, 69.56it/s]val_loss: 216120464.0\n",
      "val_loss: 215926480.0\n",
      "val_loss: 215659104.0\n",
      "val_loss: 216646256.0\n",
      "val_loss: 217361632.0\n",
      "val_loss: 216394896.0\n",
      "val_loss: 214270320.0\n",
      "val_loss: 219466896.0\n",
      "val_loss: 217575744.0\n",
      "val_loss: 216459136.0\n",
      "val_loss: 219629744.0\n",
      "val_loss: 220252480.0\n",
      "val_loss: 219489536.0\n",
      "val_loss: 217227088.0\n",
      "val_loss: 217775504.0\n",
      "val_loss: 217551392.0\n",
      "val_loss: 220706240.0\n",
      "val_loss: 217705744.0\n",
      "val_loss: 213020448.0\n",
      "val_loss: 216173264.0\n",
      "val_loss: 219690208.0\n",
      "val_loss: 220154480.0\n",
      "val_loss: 216938896.0\n",
      "val_loss: 217132080.0\n",
      "val_loss: 218845232.0\n",
      "val_loss: 218637024.0\n",
      "val_loss: 220280432.0\n",
      "val_loss: 218620976.0\n",
      "val_loss: 213423680.0\n",
      "val_loss: 217212624.0\n",
      "val_loss: 219497344.0\n",
      "val_loss: 218564256.0\n",
      "Epoch 7:   0%|          | 0/40 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "class LSTM(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super().__init__()\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)  # Batch normalization after fully connected layer\n",
    "        self.dropout1 = nn.Dropout(0.2)  # Dropout for regularization\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm1d(output_dim)\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD - x.shape: {x.shape}\")\n",
    "        out, _ = self.lstm(x)\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD - out.shape: {out.shape}\")\n",
    "        out = self.fc(out)\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD - out.shape: {out.shape}\")\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP - x.shape: {x.shape}\")\n",
    "            print(f\"TRAINING_STEP - y.shape: {y.shape}\")\n",
    "\n",
    "        y_hat = self(x)\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP - y_hat.shape: {y_hat.shape}\")\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        print('val_loss:', loss.item())  # Add this line for debugging\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "    \n",
    "\n",
    "input_dim = 10\n",
    "hidden_dim = 46\n",
    "num_layers = 2\n",
    "output_dim = 46\n",
    "model = LSTM(input_dim, hidden_dim, num_layers, output_dim)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, enable_checkpointing=False, logger=False)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5134\n",
      "bed\n",
      "prediction: [0, 7123, 418, 5134, 6558, 6459, 5429, 5831, 5230, 5812, 5666, 5727, 5038, 6163, 5402, 5282, 5127, 5359, 5429, 5362, 5275, 5382, 5438, 5482, 5568, 5609, 5632, 5609, 5562, 5580, 5553, 5469, 5386, 5424, 5401, 5429, 5285, 5293, 5306, 5409, 5399, 5347, 5341, 5290, 5380, 5350]\n",
      "gold: [0, 39413, 438, 996, 38133, 23417, 242, 39134, 612, 438, 3761, 242, 134, 873, 25484, 134, 428, 288, 1610, 102, 2146, 242, 398, 438, 5379, 417, 245, 438, 466, 438, 40847, 23219, 6232, 4111, 4027, 7309, 5134, 134, 428, 5607, 34099, 1755, 4015, 428, 398, 417]\n",
      "Prediction: <s> reviewed moneybed Intelligencefully FC toll streaming begun quarterssell Atlantic� principalkey ninth Maybe FC publication MitchellENT Acc Stone somewhat keenwith keen Lineistic 76 Christopher dramatic wounded proven FCante Make Carter Bruce WatsonaresNEWils Robinson cur\n",
      "Gold: <s>789c15cb310e83300c46e1abfc1b0bea21e8c62d5c9c044bc846764ccbed1b96373ce95b8d\n",
      "e\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'e'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gold[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m gold[i] \u001b[38;5;129;01min\u001b[39;00m prediction:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgold[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgold\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3750\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3747\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   3748\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 3750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3754\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils.py:1001\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[0;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    993\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    998\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1001\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[0;32m   1003\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m   1004\u001b[0m     }\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils.py:976\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[1;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[0;32m    974\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[1;32m--> 976\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'e'"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "for text, hex in test_loader:\n",
    "    prediction = model(text)\n",
    "    gold = hex\n",
    "\n",
    "    prediction = prediction[0].tolist()\n",
    "    prediction = [round(x) for x in prediction]\n",
    "    print(f\"prediction: {prediction}\")\n",
    "    prediction = tokenizer.decode(prediction)\n",
    "    \n",
    "    gold = gold[0].tolist()\n",
    "    gold = [round(x) for x in gold]\n",
    "    print(f\"gold: {gold}\")\n",
    "    gold = tokenizer.decode(gold)\n",
    "\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Gold: {gold}\")\n",
    "\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
