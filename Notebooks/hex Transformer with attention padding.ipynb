{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch pandas lightning trl\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 999\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Custom Dataset\n",
    "class TextHexDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['text']\n",
    "        hex_data = self.dataframe.iloc[idx]['deflate_hex']\n",
    "        return text, hex_data\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../Datasets/new_dataset_deflate.csv')\n",
    "\n",
    "# Create datasets\n",
    "dataset = TextHexDataset(df)\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders with batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION_STEP: loss = 172628288.0\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s]VALIDATION_STEP: loss = 184529344.0\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1250/1250 [16:16<00:00,  1.28it/s, v_num=42]VALIDATION_STEP: loss = 120623312.0\n",
      "VALIDATION_STEP: loss = 128157208.0\n",
      "VALIDATION_STEP: loss = 116489544.0\n",
      "VALIDATION_STEP: loss = 128303304.0\n",
      "VALIDATION_STEP: loss = 121086440.0\n",
      "VALIDATION_STEP: loss = 115566424.0\n",
      "VALIDATION_STEP: loss = 127646448.0\n",
      "VALIDATION_STEP: loss = 119994928.0\n",
      "VALIDATION_STEP: loss = 121542488.0\n",
      "VALIDATION_STEP: loss = 122171808.0\n",
      "VALIDATION_STEP: loss = 122247120.0\n",
      "VALIDATION_STEP: loss = 120634488.0\n",
      "VALIDATION_STEP: loss = 124290512.0\n",
      "VALIDATION_STEP: loss = 123832160.0\n",
      "VALIDATION_STEP: loss = 121963120.0\n",
      "VALIDATION_STEP: loss = 121057264.0\n",
      "VALIDATION_STEP: loss = 124814656.0\n",
      "VALIDATION_STEP: loss = 122875776.0\n",
      "VALIDATION_STEP: loss = 117025872.0\n",
      "VALIDATION_STEP: loss = 129087688.0\n",
      "VALIDATION_STEP: loss = 119365600.0\n",
      "VALIDATION_STEP: loss = 117803232.0\n",
      "VALIDATION_STEP: loss = 123178144.0\n",
      "VALIDATION_STEP: loss = 116724728.0\n",
      "VALIDATION_STEP: loss = 125109856.0\n",
      "VALIDATION_STEP: loss = 121503840.0\n",
      "VALIDATION_STEP: loss = 125085616.0\n",
      "VALIDATION_STEP: loss = 121580352.0\n",
      "VALIDATION_STEP: loss = 125658056.0\n",
      "VALIDATION_STEP: loss = 124628280.0\n",
      "VALIDATION_STEP: loss = 131512192.0\n",
      "VALIDATION_STEP: loss = 119462672.0\n",
      "VALIDATION_STEP: loss = 131008344.0\n",
      "VALIDATION_STEP: loss = 125930864.0\n",
      "VALIDATION_STEP: loss = 130255336.0\n",
      "VALIDATION_STEP: loss = 121369816.0\n",
      "VALIDATION_STEP: loss = 117970512.0\n",
      "VALIDATION_STEP: loss = 121869168.0\n",
      "VALIDATION_STEP: loss = 120286792.0\n",
      "VALIDATION_STEP: loss = 126429520.0\n",
      "VALIDATION_STEP: loss = 125935776.0\n",
      "VALIDATION_STEP: loss = 121311840.0\n",
      "VALIDATION_STEP: loss = 126382352.0\n",
      "VALIDATION_STEP: loss = 126038616.0\n",
      "VALIDATION_STEP: loss = 118914584.0\n",
      "VALIDATION_STEP: loss = 123081664.0\n",
      "VALIDATION_STEP: loss = 126692784.0\n",
      "VALIDATION_STEP: loss = 126618448.0\n",
      "VALIDATION_STEP: loss = 115633968.0\n",
      "VALIDATION_STEP: loss = 132367360.0\n",
      "VALIDATION_STEP: loss = 124621368.0\n",
      "VALIDATION_STEP: loss = 122954448.0\n",
      "VALIDATION_STEP: loss = 129559408.0\n",
      "VALIDATION_STEP: loss = 125580312.0\n",
      "VALIDATION_STEP: loss = 124524056.0\n",
      "VALIDATION_STEP: loss = 123499376.0\n",
      "VALIDATION_STEP: loss = 114891016.0\n",
      "VALIDATION_STEP: loss = 127502200.0\n",
      "VALIDATION_STEP: loss = 127203392.0\n",
      "VALIDATION_STEP: loss = 121138192.0\n",
      "VALIDATION_STEP: loss = 125285800.0\n",
      "VALIDATION_STEP: loss = 126033744.0\n",
      "VALIDATION_STEP: loss = 121790096.0\n",
      "VALIDATION_STEP: loss = 124056064.0\n",
      "VALIDATION_STEP: loss = 122506352.0\n",
      "VALIDATION_STEP: loss = 124930552.0\n",
      "VALIDATION_STEP: loss = 123347280.0\n",
      "VALIDATION_STEP: loss = 128008448.0\n",
      "VALIDATION_STEP: loss = 127308576.0\n",
      "VALIDATION_STEP: loss = 123769392.0\n",
      "VALIDATION_STEP: loss = 126958856.0\n",
      "VALIDATION_STEP: loss = 118603744.0\n",
      "VALIDATION_STEP: loss = 123182448.0\n",
      "VALIDATION_STEP: loss = 128376752.0\n",
      "VALIDATION_STEP: loss = 121402944.0\n",
      "VALIDATION_STEP: loss = 129506320.0\n",
      "VALIDATION_STEP: loss = 118775112.0\n",
      "VALIDATION_STEP: loss = 123353488.0\n",
      "VALIDATION_STEP: loss = 126019256.0\n",
      "VALIDATION_STEP: loss = 121618024.0\n",
      "VALIDATION_STEP: loss = 128291216.0\n",
      "VALIDATION_STEP: loss = 124349616.0\n",
      "VALIDATION_STEP: loss = 122705648.0\n",
      "VALIDATION_STEP: loss = 129142136.0\n",
      "VALIDATION_STEP: loss = 119728600.0\n",
      "VALIDATION_STEP: loss = 127212720.0\n",
      "VALIDATION_STEP: loss = 126502888.0\n",
      "VALIDATION_STEP: loss = 123710864.0\n",
      "VALIDATION_STEP: loss = 123097544.0\n",
      "VALIDATION_STEP: loss = 128460872.0\n",
      "VALIDATION_STEP: loss = 113057288.0\n",
      "VALIDATION_STEP: loss = 114130776.0\n",
      "VALIDATION_STEP: loss = 120328280.0\n",
      "VALIDATION_STEP: loss = 124849056.0\n",
      "VALIDATION_STEP: loss = 126165928.0\n",
      "VALIDATION_STEP: loss = 128455680.0\n",
      "VALIDATION_STEP: loss = 125131664.0\n",
      "VALIDATION_STEP: loss = 123915648.0\n",
      "VALIDATION_STEP: loss = 116557672.0\n",
      "VALIDATION_STEP: loss = 122858784.0\n",
      "VALIDATION_STEP: loss = 127709888.0\n",
      "VALIDATION_STEP: loss = 133439264.0\n",
      "VALIDATION_STEP: loss = 117434624.0\n",
      "VALIDATION_STEP: loss = 125228912.0\n",
      "VALIDATION_STEP: loss = 118493168.0\n",
      "VALIDATION_STEP: loss = 122016464.0\n",
      "VALIDATION_STEP: loss = 122567088.0\n",
      "VALIDATION_STEP: loss = 120014280.0\n",
      "VALIDATION_STEP: loss = 123785296.0\n",
      "VALIDATION_STEP: loss = 129452392.0\n",
      "VALIDATION_STEP: loss = 120574248.0\n",
      "VALIDATION_STEP: loss = 128680104.0\n",
      "VALIDATION_STEP: loss = 121332576.0\n",
      "VALIDATION_STEP: loss = 120777904.0\n",
      "VALIDATION_STEP: loss = 121610992.0\n",
      "VALIDATION_STEP: loss = 125142688.0\n",
      "VALIDATION_STEP: loss = 123374256.0\n",
      "VALIDATION_STEP: loss = 123669320.0\n",
      "VALIDATION_STEP: loss = 129622224.0\n",
      "VALIDATION_STEP: loss = 125931864.0\n",
      "VALIDATION_STEP: loss = 128272688.0\n",
      "VALIDATION_STEP: loss = 120383888.0\n",
      "VALIDATION_STEP: loss = 122997272.0\n",
      "VALIDATION_STEP: loss = 124635648.0\n",
      "VALIDATION_STEP: loss = 121371656.0\n",
      "VALIDATION_STEP: loss = 123530032.0\n",
      "VALIDATION_STEP: loss = 129114784.0\n",
      "VALIDATION_STEP: loss = 115736624.0\n",
      "VALIDATION_STEP: loss = 124488112.0\n",
      "VALIDATION_STEP: loss = 119665888.0\n",
      "VALIDATION_STEP: loss = 121067816.0\n",
      "VALIDATION_STEP: loss = 119726312.0\n",
      "VALIDATION_STEP: loss = 118464544.0\n",
      "VALIDATION_STEP: loss = 119085392.0\n",
      "VALIDATION_STEP: loss = 120718688.0\n",
      "VALIDATION_STEP: loss = 128009832.0\n",
      "VALIDATION_STEP: loss = 124834296.0\n",
      "VALIDATION_STEP: loss = 124345816.0\n",
      "VALIDATION_STEP: loss = 125803440.0\n",
      "VALIDATION_STEP: loss = 126073664.0\n",
      "VALIDATION_STEP: loss = 124369744.0\n",
      "VALIDATION_STEP: loss = 121807936.0\n",
      "VALIDATION_STEP: loss = 125191680.0\n",
      "VALIDATION_STEP: loss = 128085016.0\n",
      "VALIDATION_STEP: loss = 120114808.0\n",
      "VALIDATION_STEP: loss = 125520080.0\n",
      "VALIDATION_STEP: loss = 119416832.0\n",
      "VALIDATION_STEP: loss = 125769296.0\n",
      "VALIDATION_STEP: loss = 124566208.0\n",
      "VALIDATION_STEP: loss = 120610144.0\n",
      "VALIDATION_STEP: loss = 121364672.0\n",
      "VALIDATION_STEP: loss = 118817904.0\n",
      "VALIDATION_STEP: loss = 126033832.0\n",
      "VALIDATION_STEP: loss = 121892472.0\n",
      "VALIDATION_STEP: loss = 118238320.0\n",
      "VALIDATION_STEP: loss = 128096928.0\n",
      "VALIDATION_STEP: loss = 120923200.0\n",
      "VALIDATION_STEP: loss = 118620752.0\n",
      "VALIDATION_STEP: loss = 120925472.0\n",
      "VALIDATION_STEP: loss = 130368088.0\n",
      "VALIDATION_STEP: loss = 117816800.0\n",
      "VALIDATION_STEP: loss = 124024088.0\n",
      "VALIDATION_STEP: loss = 127660056.0\n",
      "VALIDATION_STEP: loss = 129123680.0\n",
      "VALIDATION_STEP: loss = 125195896.0\n",
      "VALIDATION_STEP: loss = 119012128.0\n",
      "VALIDATION_STEP: loss = 122927696.0\n",
      "VALIDATION_STEP: loss = 113211040.0\n",
      "VALIDATION_STEP: loss = 117511216.0\n",
      "VALIDATION_STEP: loss = 124266016.0\n",
      "VALIDATION_STEP: loss = 126317000.0\n",
      "VALIDATION_STEP: loss = 128758784.0\n",
      "VALIDATION_STEP: loss = 120705528.0\n",
      "VALIDATION_STEP: loss = 126856808.0\n",
      "VALIDATION_STEP: loss = 122968616.0\n",
      "VALIDATION_STEP: loss = 132543744.0\n",
      "VALIDATION_STEP: loss = 122317864.0\n",
      "VALIDATION_STEP: loss = 127397360.0\n",
      "VALIDATION_STEP: loss = 125180832.0\n",
      "VALIDATION_STEP: loss = 114077488.0\n",
      "VALIDATION_STEP: loss = 126642480.0\n",
      "VALIDATION_STEP: loss = 120005336.0\n",
      "VALIDATION_STEP: loss = 123608544.0\n",
      "VALIDATION_STEP: loss = 127554968.0\n",
      "VALIDATION_STEP: loss = 118401232.0\n",
      "VALIDATION_STEP: loss = 117436136.0\n",
      "VALIDATION_STEP: loss = 124698240.0\n",
      "VALIDATION_STEP: loss = 121576944.0\n",
      "VALIDATION_STEP: loss = 131725112.0\n",
      "VALIDATION_STEP: loss = 122484240.0\n",
      "VALIDATION_STEP: loss = 122218304.0\n",
      "VALIDATION_STEP: loss = 118524192.0\n",
      "VALIDATION_STEP: loss = 121855448.0\n",
      "VALIDATION_STEP: loss = 119798320.0\n",
      "VALIDATION_STEP: loss = 118970720.0\n",
      "VALIDATION_STEP: loss = 124916344.0\n",
      "VALIDATION_STEP: loss = 120043672.0\n",
      "VALIDATION_STEP: loss = 124145152.0\n",
      "VALIDATION_STEP: loss = 122006048.0\n",
      "VALIDATION_STEP: loss = 114378104.0\n",
      "VALIDATION_STEP: loss = 125336864.0\n",
      "VALIDATION_STEP: loss = 126685160.0\n",
      "VALIDATION_STEP: loss = 123265664.0\n",
      "VALIDATION_STEP: loss = 123474848.0\n",
      "VALIDATION_STEP: loss = 118888352.0\n",
      "VALIDATION_STEP: loss = 122542408.0\n",
      "VALIDATION_STEP: loss = 121995168.0\n",
      "VALIDATION_STEP: loss = 122649880.0\n",
      "VALIDATION_STEP: loss = 125920384.0\n",
      "VALIDATION_STEP: loss = 114807632.0\n",
      "VALIDATION_STEP: loss = 123944768.0\n",
      "VALIDATION_STEP: loss = 127194368.0\n",
      "VALIDATION_STEP: loss = 117434448.0\n",
      "VALIDATION_STEP: loss = 120241904.0\n",
      "VALIDATION_STEP: loss = 128152208.0\n",
      "VALIDATION_STEP: loss = 121101448.0\n",
      "VALIDATION_STEP: loss = 127379840.0\n",
      "VALIDATION_STEP: loss = 125380184.0\n",
      "VALIDATION_STEP: loss = 127426960.0\n",
      "VALIDATION_STEP: loss = 127961632.0\n",
      "VALIDATION_STEP: loss = 121544560.0\n",
      "VALIDATION_STEP: loss = 130835032.0\n",
      "VALIDATION_STEP: loss = 118080832.0\n",
      "VALIDATION_STEP: loss = 126701552.0\n",
      "VALIDATION_STEP: loss = 125279672.0\n",
      "VALIDATION_STEP: loss = 138141696.0\n",
      "VALIDATION_STEP: loss = 124700296.0\n",
      "VALIDATION_STEP: loss = 125120504.0\n",
      "VALIDATION_STEP: loss = 130270320.0\n",
      "VALIDATION_STEP: loss = 123065504.0\n",
      "VALIDATION_STEP: loss = 121747360.0\n",
      "VALIDATION_STEP: loss = 119680864.0\n",
      "VALIDATION_STEP: loss = 121720168.0\n",
      "VALIDATION_STEP: loss = 121708496.0\n",
      "VALIDATION_STEP: loss = 123176960.0\n",
      "VALIDATION_STEP: loss = 120461000.0\n",
      "VALIDATION_STEP: loss = 126887664.0\n",
      "VALIDATION_STEP: loss = 120223736.0\n",
      "VALIDATION_STEP: loss = 123764048.0\n",
      "VALIDATION_STEP: loss = 124508936.0\n",
      "VALIDATION_STEP: loss = 121891016.0\n",
      "VALIDATION_STEP: loss = 118579664.0\n",
      "VALIDATION_STEP: loss = 119170240.0\n",
      "VALIDATION_STEP: loss = 117562752.0\n",
      "VALIDATION_STEP: loss = 116890832.0\n",
      "VALIDATION_STEP: loss = 125446480.0\n",
      "VALIDATION_STEP: loss = 129646224.0\n",
      "VALIDATION_STEP: loss = 116093696.0\n",
      "VALIDATION_STEP: loss = 126267856.0\n",
      "VALIDATION_STEP: loss = 119527984.0\n",
      "VALIDATION_STEP: loss = 125384912.0\n",
      "VALIDATION_STEP: loss = 127672816.0\n",
      "VALIDATION_STEP: loss = 118625768.0\n",
      "VALIDATION_STEP: loss = 123003760.0\n",
      "VALIDATION_STEP: loss = 123333216.0\n",
      "VALIDATION_STEP: loss = 135498304.0\n",
      "VALIDATION_STEP: loss = 123669728.0\n",
      "VALIDATION_STEP: loss = 128472888.0\n",
      "VALIDATION_STEP: loss = 122098848.0\n",
      "VALIDATION_STEP: loss = 127348232.0\n",
      "VALIDATION_STEP: loss = 130498816.0\n",
      "VALIDATION_STEP: loss = 125930448.0\n",
      "VALIDATION_STEP: loss = 124680760.0\n",
      "VALIDATION_STEP: loss = 128259800.0\n",
      "VALIDATION_STEP: loss = 120811152.0\n",
      "VALIDATION_STEP: loss = 122152432.0\n",
      "VALIDATION_STEP: loss = 119399200.0\n",
      "VALIDATION_STEP: loss = 116591376.0\n",
      "VALIDATION_STEP: loss = 119788224.0\n",
      "VALIDATION_STEP: loss = 121937576.0\n",
      "VALIDATION_STEP: loss = 125282000.0\n",
      "VALIDATION_STEP: loss = 121714976.0\n",
      "VALIDATION_STEP: loss = 115844016.0\n",
      "VALIDATION_STEP: loss = 124950184.0\n",
      "VALIDATION_STEP: loss = 125595536.0\n",
      "VALIDATION_STEP: loss = 130017072.0\n",
      "VALIDATION_STEP: loss = 120726992.0\n",
      "VALIDATION_STEP: loss = 119431104.0\n",
      "VALIDATION_STEP: loss = 115491432.0\n",
      "VALIDATION_STEP: loss = 125724632.0\n",
      "VALIDATION_STEP: loss = 122457096.0\n",
      "VALIDATION_STEP: loss = 122102760.0\n",
      "VALIDATION_STEP: loss = 129929120.0\n",
      "VALIDATION_STEP: loss = 122539624.0\n",
      "VALIDATION_STEP: loss = 124459104.0\n",
      "VALIDATION_STEP: loss = 120859872.0\n",
      "VALIDATION_STEP: loss = 119156864.0\n",
      "VALIDATION_STEP: loss = 120641632.0\n",
      "VALIDATION_STEP: loss = 125694472.0\n",
      "VALIDATION_STEP: loss = 126611736.0\n",
      "VALIDATION_STEP: loss = 123714904.0\n",
      "VALIDATION_STEP: loss = 123216320.0\n",
      "VALIDATION_STEP: loss = 125177688.0\n",
      "VALIDATION_STEP: loss = 120559328.0\n",
      "VALIDATION_STEP: loss = 120666288.0\n",
      "VALIDATION_STEP: loss = 124453216.0\n",
      "VALIDATION_STEP: loss = 125475600.0\n",
      "VALIDATION_STEP: loss = 122395056.0\n",
      "VALIDATION_STEP: loss = 114336752.0\n",
      "VALIDATION_STEP: loss = 120841024.0\n",
      "VALIDATION_STEP: loss = 129636048.0\n",
      "VALIDATION_STEP: loss = 125775056.0\n",
      "VALIDATION_STEP: loss = 121701824.0\n",
      "VALIDATION_STEP: loss = 118272304.0\n",
      "VALIDATION_STEP: loss = 117273840.0\n",
      "VALIDATION_STEP: loss = 126377384.0\n",
      "VALIDATION_STEP: loss = 120281840.0\n",
      "VALIDATION_STEP: loss = 129312800.0\n",
      "VALIDATION_STEP: loss = 114265736.0\n",
      "VALIDATION_STEP: loss = 129622784.0\n",
      "VALIDATION_STEP: loss = 120171872.0\n",
      "VALIDATION_STEP: loss = 127364808.0\n",
      "VALIDATION_STEP: loss = 117317392.0\n",
      "Epoch 0: 100%|██████████| 1250/1250 [17:48<00:00,  1.17it/s, v_num=42]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "class TransformerModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "        self.transformer = BartModel.from_pretrained('facebook/bart-base')\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128 * 768, 128)\n",
    "\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        # Tokenize the text\n",
    "        input_ids = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        attention_mask = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).attention_mask.to(device)\n",
    "\n",
    "        # Pad input_ids and attention_mask to a fixed length of 128\n",
    "        padded_input_ids = F.pad(input_ids, (0, 128 - input_ids.shape[1]), 'constant', 0)\n",
    "        padded_attention_mask = F.pad(attention_mask, (0, 128 - attention_mask.shape[1]), 'constant', 0)\n",
    "\n",
    "        # Ensure padding is on the device\n",
    "        padded_input_ids = padded_input_ids.to(device)\n",
    "        padded_attention_mask = padded_attention_mask.to(device)\n",
    "\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: padded_input_ids.shape = {padded_input_ids.shape}\")\n",
    "            print(f\"FORWARD: padded_attention_mask.shape = {padded_attention_mask.shape}\")\n",
    "\n",
    "        # Pass tokenized and padded text through the transformer\n",
    "        transformer_output = self.transformer(input_ids=padded_input_ids, attention_mask=padded_attention_mask).last_hidden_state\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: transformer_output.shape = {transformer_output.shape}\")\n",
    "\n",
    "        # Pooling over the sequence dimension\n",
    "        flattened_output = self.flatten(transformer_output)\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: flattened_output.shape = {flattened_output.shape}\")\n",
    "\n",
    "        # Apply the linear layer\n",
    "        final_output = self.linear(flattened_output)\n",
    "        if DEBUG:\n",
    "            print(f\"FORWARD: final_output.shape = {final_output.shape}\")\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text, hex_data = batch\n",
    "\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP: text = {text}\")\n",
    "            print(f\"TRAINING_STEP: hex_data = {hex_data}\")\n",
    "\n",
    "        #encode hex_data\n",
    "        encoded_hex_data = self.tokenizer(hex_data, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP: encoded_hex_data.shape = {encoded_hex_data.shape}\")\n",
    "\n",
    "        #padd the hex_data\n",
    "        padded_encoded_hex_data = F.pad(encoded_hex_data, (0, 128 - encoded_hex_data.shape[1]), 'constant', 0)\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP: padded_encoded_hex_data.shape = {padded_encoded_hex_data.shape}\")\n",
    "\n",
    "        # Pass the text through the transformer\n",
    "        transformer_output = self.forward(text)\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP: transformer_output.shape = {transformer_output.shape}\")\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.loss(transformer_output, padded_encoded_hex_data.float())\n",
    "        if DEBUG:\n",
    "            print(f\"TRAINING_STEP: loss = {loss}\")\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text, hex_data = batch\n",
    "        if DEBUG:\n",
    "            print(f\"VALIDATION_STEP: text = {text}\")\n",
    "            print(f\"VALIDATION_STEP: hex_data = {hex_data}\")\n",
    "\n",
    "        #encode hex_data\n",
    "        encoded_hex_data = self.tokenizer(hex_data, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        if DEBUG:\n",
    "            print(f\"VALIDATION_STEP: encoded_hex_data.shape = {encoded_hex_data.shape}\")\n",
    "\n",
    "        #padd the hex_data\n",
    "        padded_encoded_hex_data = F.pad(encoded_hex_data, (0, 128 - encoded_hex_data.shape[1]), 'constant', 0)\n",
    "        if DEBUG:\n",
    "            print(f\"VALIDATION_STEP: padded_encoded_hex_data.shape = {padded_encoded_hex_data.shape}\")\n",
    "        \n",
    "        # Pass the text through the transformer\n",
    "        transformer_output = self.forward(text)\n",
    "        if DEBUG:\n",
    "            print(f\"VALIDATION_STEP: transformer_output.shape = {transformer_output.shape}\")\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.loss(transformer_output, padded_encoded_hex_data.float())\n",
    "        print(f\"VALIDATION_STEP: loss = {loss}\")\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerModel()\n",
    "trainer = pl.Trainer(max_epochs=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, \u001b[38;5;28mhex\u001b[39m \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m----> 3\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtodevice()\n\u001b[0;32m      4\u001b[0m     gold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhex\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFORWARD: padded_attention_mask.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpadded_attention_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Pass tokenized and padded text through the transformer\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadded_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadded_attention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFORWARD: transformer_output.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformer_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1596\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1593\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1596\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[0;32m   1606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1149\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1149\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_scale\n\u001b[0;32m   1151\u001b[0m embed_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_positions(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1152\u001b[0m embed_pos \u001b[38;5;241m=\u001b[39m embed_pos\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "for text, hex in val_loader:\n",
    "\n",
    "    prediction = model(text)\n",
    "    gold = hex\n",
    "\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Gold: {gold}\")\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
